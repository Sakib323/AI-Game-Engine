{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124\n!pip install transformers\n!pip install datasets\n!pip install wandb\n!pip install -U datasets\n!pip install objaverse\n!pip install diffusers\n!pip install trimesh\n!pip install jaxtyping\n!pip install pytorch-lightning\n!pip install ijson\n!pip install triton==3.2.0\n!pip install wandb\n!pip install decord\n!pip uninstall -y pillow\n!pip install pillow==9.5.0 --no-cache-dir\n!pip install yt-dlp opencv-python accelerate\n!pip install \"numpy<2.0\"","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install torch-cluster -f https://data.pyg.org/whl/torch-$(python -c \"import torch; print(torch.__version__)\").html\n!apt-get update && apt-get install -y libaio-dev","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import sys\n!rm -rf AI-Game-Engine\n!git clone https://github.com/Sakib323/AI-Game-Engine.git\nsys.path.append('/kaggle/working/AI-Game-Engine')\nfrom mmfreelm.models.hgrn_bit.video_gen import VideoDiT_models","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T09:40:40.518817Z","iopub.execute_input":"2025-11-25T09:40:40.519388Z","iopub.status.idle":"2025-11-25T09:41:09.540057Z","shell.execute_reply.started":"2025-11-25T09:40:40.519355Z","shell.execute_reply":"2025-11-25T09:41:09.539201Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'AI-Game-Engine'...\nremote: Enumerating objects: 1220, done.\u001b[K\nremote: Counting objects: 100% (225/225), done.\u001b[K\nremote: Compressing objects: 100% (131/131), done.\u001b[K\nremote: Total 1220 (delta 176), reused 138 (delta 94), pack-reused 995 (from 1)\u001b[K\nReceiving objects: 100% (1220/1220), 463.41 MiB | 50.79 MiB/s, done.\nResolving deltas: 100% (823/823), done.\nUpdating files: 100% (240/240), done.\n","output_type":"stream"},{"name":"stderr","text":"2025-11-25 09:41:00.686215: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1764063660.708651   16359 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1764063660.715440   16359 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nimport ast\nimport torch\nimport cv2\nimport numpy as np\nfrom datasets import load_dataset\nfrom yt_dlp import YoutubeDL\nfrom transformers import AutoTokenizer\nfrom diffusers import AutoencoderKLTemporalDecoder\nfrom torchvision import transforms\nfrom huggingface_hub import hf_hub_download, HfApi\nimport concurrent.futures\n\nHF_TOKEN = \"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\" \nHF_USERNAME = \"Sakib323\" \nREPO_NAME = \"panda-70m-latents\"\nREPO_ID = f\"{HF_USERNAME}/{REPO_NAME}\"\n\nDATASET_NAME = \"multimodalart/panda-70m\"\nSPLIT = \"train_2m\"\nTOKENIZER_NAME = \"Sakib323/MMfreeLM-370M\"\nOUTPUT_DIR = \"./processed_data\"\nVAE_MODEL = \"stabilityai/stable-video-diffusion-img2vid-xt\"\nMAX_FRAMES = 16 \nHEIGHT = 576\nWIDTH = 1024\nTRACKER_FILE = \"last_processed_index.txt\"\nMAX_WORKERS = 3\n\n\nUSE_COOKIES = False \n\nraw_cookie_data = \"\"\"\n# Netscape HTTP Cookie File\n# https://curl.haxx.se/rfc/cookie_spec.html\n# This is a generated file! Do not edit.\n\n.youtube.com\tTRUE\t/\tTRUE\t1763996651\tGPS\t1\n.youtube.com\tTRUE\t/\tTRUE\t1798554949\tPREF\tf4=4000000&f6=40000000&tz=Asia.Dhaka\n.youtube.com\tTRUE\t/\tTRUE\t1795530946\t__Secure-1PSIDTS\tsidts-CjUBwQ9iI5iFgsUBtEw5vAH3nuwe02XMI5CivA4z8_UoID1gnZomh-cBc1pW3POcwhFwBGF49RAA\n.youtube.com\tTRUE\t/\tTRUE\t1795530946\t__Secure-3PSIDTS\tsidts-CjUBwQ9iI5iFgsUBtEw5vAH3nuwe02XMI5CivA4z8_UoID1gnZomh-cBc1pW3POcwhFwBGF49RAA\n.youtube.com\tTRUE\t/\tFALSE\t1798554946\tHSID\tApH6tDZwvbaKYnkg5\n.youtube.com\tTRUE\t/\tTRUE\t1798554946\tSSID\tAjpquhY5hm5coqSae\n.youtube.com\tTRUE\t/\tFALSE\t1798554946\tAPISID\teS6pEBbnrj4Oy5R0/Aac7D4e3Hj_60HPAB\n.youtube.com\tTRUE\t/\tTRUE\t1798554946\tSAPISID\tZUPk8bl52LXZ_EU6/AVyk78JOYu4zRCQix\n.youtube.com\tTRUE\t/\tTRUE\t1798554946\t__Secure-1PAPISID\tZUPk8bl52LXZ_EU6/AVyk78JOYu4zRCQix\n.youtube.com\tTRUE\t/\tTRUE\t1798554946\t__Secure-3PAPISID\tZUPk8bl52LXZ_EU6/AVyk78JOYu4zRCQix\n.youtube.com\tTRUE\t/\tFALSE\t1798554946\tSID\tg.a0003whnnXO3ul1IzRb0idHCJkkBuYmKwR5GAacTq76hIgIq2xwWGpgS8GRxAdb2cm76UxmgZgACgYKAdYSARASFQHGX2MiQHcRCfOFeO9o0HcnV87jdRoVAUF8yKpCz2-PWeLEHXx8-EmC9HS80076\n.youtube.com\tTRUE\t/\tTRUE\t1798554946\t__Secure-1PSID\tg.a0003whnnXO3ul1IzRb0idHCJkkBuYmKwR5GAacTq76hIgIq2xwWQbDbsdbnIM5_s7x1vomBwwACgYKAXMSARASFQHGX2MizQxZbVxUdHtaRDHLvfC7GxoVAUF8yKqtjRgAWqZihboY-zFNwrqy0076\n.youtube.com\tTRUE\t/\tTRUE\t1798554946\t__Secure-3PSID\tg.a0003whnnXO3ul1IzRb0idHCJkkBuYmKwR5GAacTq76hIgIq2xwWM1VwjuglrGlIMCW3lE3JJQACgYKAbgSARASFQHGX2MignvvLfSxhIcbYqabYbP-0xoVAUF8yKrkz30ttTVR8Ai5vLb0hHud0076\n.youtube.com\tTRUE\t/\tTRUE\t1798554946\tLOGIN_INFO\tAFmmF2swRQIhALx3ex64ZTk1SltaPOYOcUqWL-V7ojwZ7NHbE7g5iqyDAiBkcbOe3i1SkmgP_qXf9O_u3wgXCXgFyTmRLCFHhsuisg:QUQ3MjNmeGtlQ3FCUWtHZ3dReFpHdmVLSWNnbzFQVk9pWlZFRlBmTVo1TTlaLTBQYlhmTWxMcUJKQUQ2R2lZLS1oX1JWTXpkS3NxZ1ZVQS1OYUVsYW45R1dCWG9HWXMwMy1GSGhhTDRVOXYwd3AxcmZLVEdIMXRnbXBtLVNqV3J0MkE1bEQyWWh1Z3c4YXNPSFBqSzNvTmRJOW8teVpzazNR\n.youtube.com\tTRUE\t/\tFALSE\t1763994954\tST-xuwub9\tsession_logininfo=AFmmF2swRQIhALx3ex64ZTk1SltaPOYOcUqWL-V7ojwZ7NHbE7g5iqyDAiBkcbOe3i1SkmgP_qXf9O_u3wgXCXgFyTmRLCFHhsuisg%3AQUQ3MjNmeGtlQ3FCUWtHZ3dReFpHdmVLSWNnbzFQVk9pWlZFRlBmTVo1TTlaLTBQYlhmTWxMcUJKQUQ2R2lZLS1oX1JWTXpkS3NxZ1ZVQS1OYUVsYW45R1dCWG9HWXMwMy1GSGhhTDRVOXYwd3AxcmZLVEdIMXRnbXBtLVNqV3J0MkE1bEQyWWh1Z3c4YXNPSFBqSzNvTmRJOW8teVpzazNR\n.youtube.com\tTRUE\t/\tFALSE\t1795530952\tSIDCC\tAKEyXzUJL0EENuNOtPMIJq0ZUgMqXZkvDINL-h1ebOTxTg3dPH9UvGl-AUmkAsVZcp8Yt2gK\n.youtube.com\tTRUE\t/\tTRUE\t1795530952\t__Secure-1PSIDCC\tAKEyXzUci307jbRdFAjsoQrrx7E1BX1AvBRJwGQVldGOuQUlX5y_H3kXzpXsudwImvj8C8nG0g\n.youtube.com\tTRUE\t/\tTRUE\t1795530952\t__Secure-3PSIDCC\tAKEyXzX5tj9bo8i-K7VhCOxvihSBWzVWT1zc5XN4wWAeEpyxHj5sW-2-w3FWJNhiDaOFo4xCBg\n.youtube.com\tTRUE\t/\tTRUE\t0\tYSC\thtup1kLd28o\n.youtube.com\tTRUE\t/\tTRUE\t1779546952\tVISITOR_INFO1_LIVE\tsbOd5HCMyqU\n.youtube.com\tTRUE\t/\tTRUE\t1779546952\tVISITOR_PRIVACY_METADATA\tCgJCRBIEGgAgNA%3D%3D\n.youtube.com\tTRUE\t/\tTRUE\t1779546853\t__Secure-ROLLOUT_TOKEN\tCMqnl7mRscKerAEQgPiR9YCLkQMY7rzx9YCLkQM%3D\n\"\"\"\n\n# --- COOKIE FILE GENERATION (We generate it anyway, but only use if toggled) ---\nprint(\"üç™ Reconstructing cookies.txt...\")\nwith open(\"cookies.txt\", \"w\", encoding=\"utf-8\") as f:\n    f.write(\"# Netscape HTTP Cookie File\\n\")\n    f.write(\"# https://curl.haxx.se/rfc/cookie_spec.html\\n\")\n    f.write(\"# This is a generated file! Do not edit.\\n\\n\")\n    for line in raw_cookie_data.strip().split('\\n'):\n        if line.strip() and not line.startswith('#'):\n            parts = line.split()\n            if len(parts) >= 7:\n                f.write(\"\\t\".join(parts) + \"\\n\")\n\n# --- MAIN SETUP ---\nos.makedirs(OUTPUT_DIR, exist_ok=True)\nstart_index = 0\napi = HfApi(token=HF_TOKEN)\n\nprint(f\"üîå Connecting to Hugging Face Repo: {REPO_ID}...\")\ntry:\n    api.create_repo(repo_id=REPO_ID, repo_type=\"dataset\", exist_ok=True)\n    local_tracker_path = hf_hub_download(\n        repo_id=REPO_ID, filename=TRACKER_FILE, repo_type=\"dataset\",\n        token=HF_TOKEN, local_dir=\"./\"\n    )\n    with open(local_tracker_path, 'r') as f:\n        start_index = int(f.read().strip())\n    print(f\"üîÑ Resuming from index: {start_index}\")\nexcept Exception as e:\n    print(f\"‚ö†Ô∏è Tracker not found. Starting from index 0.\")\n    start_index = 0\n\nprint(\"Loading Models...\")\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\ntokenizer = AutoTokenizer.from_pretrained(TOKENIZER_NAME)\ntokenizer.pad_token = tokenizer.eos_token\n\nvae = AutoencoderKLTemporalDecoder.from_pretrained(\n    VAE_MODEL, subfolder=\"vae\", \n    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32\n).to(device)\nvae.eval()\n\ndef parse_timestamp(ts_str):\n    parts = ts_str.split(':')\n    seconds = 0.0\n    for part in parts:\n        seconds = seconds * 60 + float(part)\n    return seconds\n\ndef download_task(task_data):\n    url, start_sec, end_sec, video_filename, meta_data = task_data\n    \n    if os.path.exists(meta_data['save_path']):\n        return None \n\n    ydl_opts = {\n        'format': 'bestvideo[height<=576][ext=mp4]+bestaudio[ext=m4a]/best[ext=mp4]/best',\n        'outtmpl': video_filename,\n        'quiet': True, 'no_warnings': True,\n        'extractor_args': {'youtube': {'player_client': ['default', 'ios']}},\n        'external_downloader': 'ffmpeg',\n        'external_downloader_args': {'ffmpeg_i': ['-ss', str(start_sec), '-to', str(end_sec)]},\n    }\n\n    # --- TOGGLE LOGIC ---\n    if USE_COOKIES:\n        ydl_opts['cookiefile'] = 'cookies.txt'\n\n    try:\n        with YoutubeDL(ydl_opts) as ydl:\n            ydl.download([url])\n        \n        if os.path.exists(video_filename):\n            return video_filename, meta_data\n    except Exception:\n        pass\n        \n    return None\n\ndef preprocess_video(video_path, max_frames=16, height=576, width=1024):\n    cap = cv2.VideoCapture(video_path)\n    frames = []\n    while True:\n        ret, frame = cap.read()\n        if not ret: break\n        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n        frames.append(frame)\n        if len(frames) >= max_frames: break\n    cap.release()\n    \n    if len(frames) < 1: return None\n    \n    video_np = np.array(frames) \n    video_tensor = torch.tensor(video_np).permute(0, 3, 1, 2)\n    \n    transform = transforms.Compose([\n        transforms.Resize((height, width)),\n        transforms.Lambda(lambda x: x / 127.5 - 1.0)\n    ])\n    return torch.stack([transform(f) for f in video_tensor])\n\ndef clip_generator(dataset, start_idx):\n    if start_idx > 0:\n        print(f\"‚è© Skipping first {start_idx} rows...\")\n        dataset = dataset.skip(start_idx)\n\n    for i, row in enumerate(dataset):\n        current_global_index = start_idx + i\n        \n        if i % 50 == 0:\n            with open(os.path.join(OUTPUT_DIR, TRACKER_FILE), 'w') as f:\n                f.write(str(current_global_index))\n\n        try:\n            video_id = row['videoID']\n            url = row['url']\n            timestamps = ast.literal_eval(row['timestamp'])\n            captions = ast.literal_eval(row['caption'])\n\n            for idx, (time_range, caption) in enumerate(zip(timestamps, captions)):\n                start_sec = parse_timestamp(time_range[0])\n                end_sec = parse_timestamp(time_range[1])\n                \n                clip_name = f\"{video_id}_{idx}\"\n                video_filename = f\"{clip_name}.mp4\"\n                save_path = os.path.join(OUTPUT_DIR, f\"{clip_name}.pt\")\n                \n                meta = {\n                    'clip_name': clip_name,\n                    'save_path': save_path,\n                    'caption': caption,\n                    'global_idx': current_global_index\n                }\n                \n                yield (url, start_sec, end_sec, video_filename, meta)\n                \n        except Exception as e:\n            print(f\"Skipping row {current_global_index}: {e}\")\n            continue\n\nprint(f\"üöÄ Starting Parallel Processing (Workers: {MAX_WORKERS})...\")\nprint(f\"üç™ Cookie Usage: {'ENABLED' if USE_COOKIES else 'DISABLED'}\")\n\ndataset_stream = load_dataset(DATASET_NAME, split=SPLIT, streaming=True)\ngen = clip_generator(dataset_stream, start_index)\n\nwith concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n    futures = {}\n\n    for _ in range(MAX_WORKERS * 2):\n        try:\n            task = next(gen)\n            future = executor.submit(download_task, task)\n            futures[future] = task\n        except StopIteration:\n            break\n\n    while futures:\n        done, _ = concurrent.futures.wait(futures, return_when=concurrent.futures.FIRST_COMPLETED)\n        \n        for future in done:\n            try:\n                result = future.result()\n                \n                if result:\n                    video_filename, meta = result\n                    clip_name = meta['clip_name']\n                    \n                    try:\n                        pixel_values = preprocess_video(video_filename, max_frames=MAX_FRAMES, height=HEIGHT, width=WIDTH)\n                        \n                        if pixel_values is not None:\n                            pixel_values = pixel_values.to(device, dtype=torch.float16 if device==\"cuda\" else torch.float32)\n                            with torch.no_grad():\n                                latents = vae.encode(pixel_values).latent_dist.mode()\n                            \n                            text_tokens = tokenizer(meta['caption'], padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"pt\")\n                            \n                            data_point = {\n                                \"video_latents\": latents.cpu(),\n                                \"input_ids\": text_tokens[\"input_ids\"].cpu(),\n                                \"attention_mask\": text_tokens[\"attention_mask\"].cpu(),\n                                \"caption\": meta['caption']\n                            }\n                            torch.save(data_point, meta['save_path'])\n                            print(f\"[{meta['global_idx']}] Processed: {clip_name}\")\n                            \n                    except Exception as e:\n                        print(f\"Encoding Error {clip_name}: {e}\")\n                    finally:\n                        if os.path.exists(video_filename):\n                            os.remove(video_filename)\n\n                try:\n                    new_task = next(gen)\n                    new_future = executor.submit(download_task, new_task)\n                    futures[new_future] = new_task\n                except StopIteration:\n                    pass\n                    \n            except Exception as e:\n                print(f\"Worker Error: {e}\")\n            del futures[future]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport torch\nimport shutil\nfrom tqdm import tqdm\nimport math\n\ndef merge_pt_files(source_dir, output_dir, chunk_size=500):\n    if not os.path.exists(source_dir):\n        print(\"Source directory not found.\")\n        return\n\n    os.makedirs(output_dir, exist_ok=True)\n    \n    # 1. Move the tracker file to output\n    tracker_file = \"last_processed_index.txt\"\n    tracker_src = os.path.join(source_dir, tracker_file)\n    tracker_dst = os.path.join(output_dir, tracker_file)\n    \n    current_index = 0\n    if os.path.exists(tracker_src):\n        with open(tracker_src, 'r') as f:\n            current_index = int(f.read().strip())\n        shutil.copy2(tracker_src, tracker_dst)\n        \n    # 2. Gather files\n    all_files = [f for f in os.listdir(source_dir) if f.endswith('.pt')]\n    total_files = len(all_files)\n    \n    if total_files == 0:\n        print(\"No files to merge.\")\n        return\n        \n    num_chunks = math.ceil(total_files / chunk_size)\n    print(f\"Merging {total_files} files into {num_chunks} chunks...\")\n\n    for i in range(num_chunks):\n        start_idx = i * chunk_size\n        end_idx = min((i + 1) * chunk_size, total_files)\n        batch_files = all_files[start_idx:end_idx]\n        \n        merged_data = []\n        for filename in tqdm(batch_files, desc=f\"Chunk {i+1}\"):\n            try:\n                data = torch.load(os.path.join(source_dir, filename))\n                merged_data.append(data)\n            except: pass\n        output_filename = f\"data_chunk_idx_{current_index}_part_{i}.pt\"\n        output_path = os.path.join(output_dir, output_filename)\n        \n        torch.save(merged_data, output_path)\n        print(f\"Saved: {output_filename}\")\n        \n        # Cleanup source files to save space\n        for filename in batch_files:\n            os.remove(os.path.join(source_dir, filename))\n            \n    print(\"Merge complete.\")\n\nif __name__ == \"__main__\":\n    SOURCE_DIR = \"processed_data\" \n    OUTPUT_DIR = \"./to_upload\"\n    CHUNK_SIZE = 500\n    merge_pt_files(SOURCE_DIR, OUTPUT_DIR, CHUNK_SIZE)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from huggingface_hub import HfApi\n\nHF_TOKEN = \"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\" \nHF_USERNAME = \"Sakib323\"\nREPO_NAME = \"panda-70m-latents\"\nREPO_ID = f\"{HF_USERNAME}/{REPO_NAME}\"\nUPLOAD_DIR = \"./to_upload\"\n\napi = HfApi(token=HF_TOKEN)\n\nprint(f\"üöÄ Uploading new chunks to {REPO_ID}...\")\n\ntry:\n    api.upload_folder(\n        folder_path=UPLOAD_DIR,\n        repo_id=REPO_ID,\n        repo_type=\"dataset\",\n        path_in_repo=\".\",  # Upload to root of repo\n        commit_message=f\"Add chunks, current index updated\"\n    )\n    print(\"‚úÖ Upload Successful!\")\n    \n    # Clean up local upload folder to save space for next run\n    import shutil\n    shutil.rmtree(UPLOAD_DIR)\n    print(\"üßπ Cleaned up local upload directory.\")\n    \nexcept Exception as e:\n    print(f\"‚ùå Upload Failed: {e}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport gc\nimport glob\nimport bisect\nimport torch\nfrom torch.utils.data import Dataset\nfrom transformers import (\n    AutoTokenizer, \n    Trainer, \n    TrainingArguments, \n    default_data_collator\n)\nfrom huggingface_hub import snapshot_download\nfrom tqdm import tqdm\nimport wandb\nfrom mmfreelm.models.hgrn_bit.video_gen import VideoDiT_models, flow_matching_loss\n\n# --- Setup ---\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\ntorch.cuda.empty_cache()\ngc.collect()\n\nMODEL_SAVE_DIR = \"/kaggle/working/checkpoints\"\nTOKENIZER_NAME = \"Sakib323/MMfreeLM-370M\"\nHF_DATASET_ID = \"Sakib323/panda-70m-latents\"\n\n# --- CONFIG ---\nBATCH_SIZE = 1 \nGRADIENT_ACCUMULATION_STEPS = 8  \nLEARNING_RATE = 5e-5             \nNUM_EPOCHS = 2\nNUM_WORKERS = 2\nINPUT_SIZE = (16, 72, 128) \nPATCH_SIZE = (1, 2, 2)\n\n# WandB Login\nWANDB_TOKEN = \"89b06c10468af620747b4bd340f72fa5d56f6849\"\nwandb.login(key=WANDB_TOKEN)\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nos.makedirs(MODEL_SAVE_DIR, exist_ok=True)\n\n# --- 1. Download Data from Hub ---\nprint(f\"üì• Downloading {HF_DATASET_ID} to local cache...\")\n# This downloads the .pt files to a local folder managed by HF\nlocal_data_dir = snapshot_download(\n    repo_id=HF_DATASET_ID, \n    repo_type=\"dataset\",\n    token=WANDB_TOKEN, # Using the same token variable if it's your HF token too, otherwise use HF token\n    allow_patterns=[\"*.pt\"] # Only download the data files\n)\nprint(f\"‚úÖ Data downloaded to: {local_data_dir}\")\n\n# --- 2. Robust Dataset Class (For Local .pt Files) ---\nclass VideoLatentDataset(Dataset):\n    def __init__(self, data_dir):\n        self.files = sorted(glob.glob(os.path.join(data_dir, \"*.pt\")))\n        self.scale_factor = 0.18215 \n        \n        # Build index map for chunks\n        self.file_map = []     \n        self.file_starts = []  \n        self.total_samples = 0\n        \n        print(f\"Scanning {len(self.files)} files to build index map...\")\n        for f_path in tqdm(self.files):\n            try:\n                # Fast check: load cpu only to get length\n                data = torch.load(f_path, map_location=\"cpu\")\n                if isinstance(data, list):\n                    count = len(data)\n                    is_list = True\n                else:\n                    count = 1\n                    is_list = False\n                \n                self.file_starts.append(self.total_samples)\n                self.file_map.append({\"path\": f_path, \"is_list\": is_list})\n                self.total_samples += count\n                del data\n            except Exception as e:\n                print(f\"Skipping broken file {f_path}: {e}\")\n                \n        print(f\"Total samples found: {self.total_samples}\")\n\n    def __len__(self):\n        return self.total_samples\n\n    def __getitem__(self, idx):\n        # Find file index\n        file_idx = bisect.bisect_right(self.file_starts, idx) - 1\n        path_info = self.file_map[file_idx]\n        start_idx = self.file_starts[file_idx]\n        \n        # Load data\n        item_data = torch.load(path_info[\"path\"], map_location=\"cpu\")\n        \n        # Extract item\n        if path_info[\"is_list\"]:\n            local_idx = idx - start_idx\n            item = item_data[local_idx]\n        else:\n            item = item_data\n            \n        # Prepare tensors\n        latents = item[\"video_latents\"].float()\n        latents = latents.permute(1, 0, 2, 3) \n        latents = latents * self.scale_factor\n        \n        return {\n            \"latents\": latents,\n            \"input_ids\": item[\"input_ids\"].squeeze(0),\n            \"attention_mask\": item[\"attention_mask\"].squeeze(0)\n        }\n\n# --- 3. Fixed Trainer ---\nclass FlowMatchingTrainer(Trainer):\n    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n        x_1 = inputs[\"latents\"]\n        cond_y = {\n            \"input_ids\": inputs[\"input_ids\"],\n            \"attention_mask\": inputs[\"attention_mask\"]\n        }\n        \n        b = x_1.shape[0]\n        device = x_1.device\n        x_0 = torch.randn_like(x_1)\n        t_step = torch.rand(b, device=device)\n        t_expand = t_step.view(b, 1, 1, 1, 1)\n        x_t = t_expand * x_1 + (1 - t_expand) * x_0\n        v_target = x_1 - x_0\n        v_pred = model(x_t, t_step, cond_y)\n        loss = F.mse_loss(v_pred, v_target)\n\n        if torch.isnan(loss) or torch.isinf(loss):\n            loss = torch.tensor(0.0, device=device, requires_grad=True)\n\n        return (loss, v_pred) if return_outputs else loss\n\n# --- 4. Execution ---\nprint(\"Initializing Dataset...\")\n# Point the dataset to the downloaded snapshot folder\nfull_dataset = VideoLatentDataset(local_data_dir)\n\nif len(full_dataset) == 0:\n    raise ValueError(\"Dataset is empty! Check your Hugging Face repo contains .pt files.\")\n\ntrain_size = int(0.9 * len(full_dataset))\neval_size = len(full_dataset) - train_size\ngenerator = torch.Generator().manual_seed(42)\ntrain_dataset, eval_dataset = torch.utils.data.random_split(full_dataset, [train_size, eval_size], generator=generator)\n\nprint(f\"Train: {len(train_dataset)} | Eval: {len(eval_dataset)}\")\n\nprint(\"Loading Tokenizer...\")\ntokenizer = AutoTokenizer.from_pretrained(TOKENIZER_NAME)\n\nos.environ[\"WANDB_PROJECT\"] = \"video-dit-3d-generation\"\n\nprint(\"Initializing VideoDiT Model...\")\nmodel = VideoDiT_models['VideoDiT-S'](\n    input_size=INPUT_SIZE,\n    patch_size=PATCH_SIZE,\n    in_channels=4, \n    vocab_size=tokenizer.vocab_size,\n    use_rope=True,\n    use_ternary_rope=True,\n    first_frame_condition=False,\n    full_precision=True,\n    optimized_bitlinear=False,\n    use_temporal=False,\n    use_grid=False,\n    use_resampling=False,\n)\n\ntraining_args = TrainingArguments(\n    output_dir=MODEL_SAVE_DIR,\n    num_train_epochs=NUM_EPOCHS,\n    per_device_train_batch_size=BATCH_SIZE,\n    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n    learning_rate=LEARNING_RATE,\n    warmup_ratio=0.1,\n    max_grad_norm=1.0,           \n    lr_scheduler_type=\"cosine\",  \n    weight_decay=0.01,\n    fp16=True,                   \n    eval_strategy=\"steps\",\n    eval_steps=500,\n    save_strategy=\"steps\",\n    save_steps=500,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_loss\",    \n    dataloader_num_workers=NUM_WORKERS,\n    dataloader_pin_memory=True,\n    save_total_limit=2,\n    logging_steps=10,\n    report_to=\"wandb\",\n    run_name=\"VideoDiT-S-HF-Load\",\n    remove_unused_columns=False,\n    label_names=[\"latents\"],\n)\n\ntrainer = FlowMatchingTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    data_collator=default_data_collator,\n)\n\nprint(\"Starting Stable Training...\")\ntrainer.train()\ntrainer.save_model(os.path.join(MODEL_SAVE_DIR, \"final_model\"))\nprint(\"Training complete.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T09:44:04.954329Z","iopub.execute_input":"2025-11-25T09:44:04.955066Z","iopub.status.idle":"2025-11-25T09:46:18.892571Z","shell.execute_reply.started":"2025-11-25T09:44:04.955037Z","shell.execute_reply":"2025-11-25T09:46:18.891215Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"name":"stdout","text":"üì• Downloading Sakib323/panda-70m-latents to local cache...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Fetching 28 files:   0%|          | 0/28 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"95b45774956c4c409936c5a800004d37"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data_chunk_idx_2410_part_2.pt:   0%|          | 0.00/581M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2622213936b649d883fc3818ee175622"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data_chunk_idx_1360_part_1.pt:   0%|          | 0.00/575M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3abb6f0a7cc34e7a92384ed71df7ab01"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data_chunk_idx_160_part_0.pt:   0%|          | 0.00/529M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7a94c6fb873b449298a33fabd05ac480"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data_chunk_idx_2410_part_0.pt:   0%|          | 0.00/582M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2f975ef5d34c44d1acf726e04d31f55a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data_chunk_idx_1360_part_0.pt:   0%|          | 0.00/576M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"97294c8f21e045dc88ce2c64e91a2cab"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data_chunk_idx_2410_part_1.pt:   0%|          | 0.00/576M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cea33ee39a384bce94c8930cedb0d3cb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data_chunk_idx_1360_part_2.pt:   0%|          | 0.00/581M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"725671e7e0a34a1aa86aaa2aea465ded"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data_chunk_idx_1360_part_3.pt:   0%|          | 0.00/24.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"de674ffcd1af4c8794d9546ac2b329dc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data_chunk_idx_2410_part_3.pt:   0%|          | 0.00/177M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"122d0cbb87f54a3aa6db91839e77384d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data_chunk_idx_3400_part_0.pt:   0%|          | 0.00/591M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8a45ce90901c40098e4741ea6c8b3959"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data_chunk_idx_3400_part_1.pt:   0%|          | 0.00/590M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"63ab5ca23c164e2882235e0c920f3c8c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data_chunk_idx_3400_part_2.pt:   0%|          | 0.00/270M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1772aaed4f71439e9d922550780725b6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data_chunk_idx_4350_part_0.pt:   0%|          | 0.00/583M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"81a04aae334042f3b195cfda6df8ff27"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data_chunk_idx_4350_part_2.pt:   0%|          | 0.00/579M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e99c75ecf47b4985b93a21e071143016"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data_chunk_idx_4350_part_1.pt:   0%|          | 0.00/581M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"953ed258ec764f2b926689031945e660"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data_chunk_idx_4350_part_3.pt:   0%|          | 0.00/583M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"427a6c2e15e34df784429c0c6f0013df"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data_chunk_idx_4350_part_4.pt:   0%|          | 0.00/294M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bba01d5b4a574ebf920c61d5e01332dd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data_chunk_idx_5200_part_0.pt:   0%|          | 0.00/584M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2b05f22e1cee42f88d96564d4d042a91"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data_chunk_idx_5200_part_1.pt:   0%|          | 0.00/576M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a27abbbbda5c42d4a73780437cf1dcae"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data_chunk_idx_5200_part_2.pt:   0%|          | 0.00/85.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7ce6cd348f414fd1899350b0660737a1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data_chunk_idx_5800_part_0.pt:   0%|          | 0.00/591M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3789ac69161e4a5ca8135f79f252eb7c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data_chunk_idx_5800_part_1.pt:   0%|          | 0.00/556M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4ebb923835d84c18b06c3d8e2061cbb2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data_chunk_idx_6350_part_0.pt:   0%|          | 0.00/591M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ebed228236e04745ad53e5470cb36e38"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data_chunk_idx_6350_part_1.pt:   0%|          | 0.00/591M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ce32bb0e8a0a4d81ad96dfc3704ffa72"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data_chunk_idx_6350_part_2.pt:   0%|          | 0.00/369M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6a909135379847439805e18f828dbe4a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data_chunk_idx_710_part_0.pt:   0%|          | 0.00/591M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"918470af3fcc4bbabe64ac1ef28852a6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data_chunk_idx_710_part_1.pt:   0%|          | 0.00/591M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"27b36c5109714635ba09c617f4b2e3bf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data_chunk_idx_710_part_2.pt:   0%|          | 0.00/167M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"757ba79a66eb4549bd365b152d0b58f0"}},"metadata":{}},{"name":"stdout","text":"‚úÖ Data downloaded to: /root/.cache/huggingface/hub/datasets--Sakib323--panda-70m-latents/snapshots/50f4d6d292a176fee09d631edded256e01f6c0e4\nInitializing Dataset...\nScanning 28 files to build index map...\n","output_type":"stream"},{"name":"stderr","text":"100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28/28 [00:12<00:00,  2.22it/s]\n","output_type":"stream"},{"name":"stdout","text":"Total samples found: 11597\nTrain: 10437 | Eval: 1160\nLoading Tokenizer...\nInitializing VideoDiT Model...\nInitializing RotaryEmbedding with theta=10000.0 and ternary=True\n\n[RotaryEmbedding] Initialized with: dim=64, max_pos=2048, base=10000.0, ternary=True\n\nInitializing RotaryEmbedding with theta=10000.0 and ternary=True\n\n[RotaryEmbedding] Initialized with: dim=64, max_pos=2048, base=10000.0, ternary=True\n\nInitializing RotaryEmbedding with theta=10000.0 and ternary=True\n\n[RotaryEmbedding] Initialized with: dim=64, max_pos=2048, base=10000.0, ternary=True\n\nInitializing RotaryEmbedding with theta=10000.0 and ternary=True\n\n[RotaryEmbedding] Initialized with: dim=64, max_pos=2048, base=10000.0, ternary=True\n\nInitializing RotaryEmbedding with theta=10000.0 and ternary=True\n\n[RotaryEmbedding] Initialized with: dim=64, max_pos=2048, base=10000.0, ternary=True\n\nInitializing RotaryEmbedding with theta=10000.0 and ternary=True\n\n[RotaryEmbedding] Initialized with: dim=64, max_pos=2048, base=10000.0, ternary=True\n\nInitializing RotaryEmbedding with theta=10000.0 and ternary=True\n\n[RotaryEmbedding] Initialized with: dim=64, max_pos=2048, base=10000.0, ternary=True\n\nInitializing RotaryEmbedding with theta=10000.0 and ternary=True\n\n[RotaryEmbedding] Initialized with: dim=64, max_pos=2048, base=10000.0, ternary=True\n\nInitializing RotaryEmbedding with theta=10000.0 and ternary=True\n\n[RotaryEmbedding] Initialized with: dim=64, max_pos=2048, base=10000.0, ternary=True\n\nInitializing RotaryEmbedding with theta=10000.0 and ternary=True\n\n[RotaryEmbedding] Initialized with: dim=64, max_pos=2048, base=10000.0, ternary=True\n\nInitializing RotaryEmbedding with theta=10000.0 and ternary=True\n\n[RotaryEmbedding] Initialized with: dim=64, max_pos=2048, base=10000.0, ternary=True\n\nInitializing RotaryEmbedding with theta=10000.0 and ternary=True\n\n[RotaryEmbedding] Initialized with: dim=64, max_pos=2048, base=10000.0, ternary=True\n\nInitializing RotaryEmbedding with theta=10000.0 and ternary=True\n\n[RotaryEmbedding] Initialized with: dim=64, max_pos=2048, base=10000.0, ternary=True\n\nInitializing RotaryEmbedding with theta=10000.0 and ternary=True\n\n[RotaryEmbedding] Initialized with: dim=64, max_pos=2048, base=10000.0, ternary=True\n\nInitializing RotaryEmbedding with theta=10000.0 and ternary=True\n\n[RotaryEmbedding] Initialized with: dim=64, max_pos=2048, base=10000.0, ternary=True\n\nInitializing RotaryEmbedding with theta=10000.0 and ternary=True\n\n[RotaryEmbedding] Initialized with: dim=64, max_pos=2048, base=10000.0, ternary=True\n\nInitializing RotaryEmbedding with theta=10000.0 and ternary=True\n\n[RotaryEmbedding] Initialized with: dim=64, max_pos=2048, base=10000.0, ternary=True\n\nInitializing RotaryEmbedding with theta=10000.0 and ternary=True\n\n[RotaryEmbedding] Initialized with: dim=64, max_pos=2048, base=10000.0, ternary=True\n\nStarting Stable Training...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.21.0"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20251125_094505-izw82bqu</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/sakibahmed2018go/video-dit-3d-generation/runs/izw82bqu' target=\"_blank\">VideoDiT-S-HF-Load</a></strong> to <a href='https://wandb.ai/sakibahmed2018go/video-dit-3d-generation' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/sakibahmed2018go/video-dit-3d-generation' target=\"_blank\">https://wandb.ai/sakibahmed2018go/video-dit-3d-generation</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/sakibahmed2018go/video-dit-3d-generation/runs/izw82bqu' target=\"_blank\">https://wandb.ai/sakibahmed2018go/video-dit-3d-generation/runs/izw82bqu</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2' max='2610' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [   2/2610 : < :, Epoch 0.00/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_16359/4216742430.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Starting Stable Training...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMODEL_SAVE_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"final_model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training complete.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2204\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2205\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2206\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2207\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2208\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2546\u001b[0m                     )\n\u001b[1;32m   2547\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2548\u001b[0;31m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2549\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2550\u001b[0m                     if (\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3747\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3748\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss_context_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3749\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3750\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3751\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_16359/4216742430.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0mx_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt_expand\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mx_1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt_expand\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mx_0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0mv_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mx_0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         \u001b[0mv_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcond_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv_target\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/utils/operations.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    816\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    817\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 818\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    819\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m     \u001b[0;31m# To act like a decorator so that it can be popped when doing `extract_model_from_parallel`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/utils/operations.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    804\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    805\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 806\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mconvert_to_fp32\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    807\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    808\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/amp/autocast_mode.py\u001b[0m in \u001b[0;36mdecorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_autocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mautocast_instance\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mdecorate_autocast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__script_unsupported\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"@autocast() decorator is not supported in script mode\"\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/kaggle/working/AI-Game-Engine/mmfreelm/models/hgrn_bit/video_gen.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, t, y, first_frame)\u001b[0m\n\u001b[1;32m    637\u001b[0m         \u001b[0mcombined_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_tokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mblock\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msingle_stream_blocks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 639\u001b[0;31m             \u001b[0mcombined_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombined_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_emb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[0mprocessed_x_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcombined_tokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mnum_x_tokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/kaggle/working/AI-Game-Engine/mmfreelm/models/hgrn_bit/video_gen.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, c)\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mgate_msa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mattn_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m         \u001b[0mmlp_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshift_mlp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_mlp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 492\u001b[0;31m         \u001b[0mmlp_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmlp_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    493\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mgate_mlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmlp_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/kaggle/working/AI-Game-Engine/mmfreelm/models/hgrn_bit/video_gen.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgate_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0mgate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdown_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgate\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 136.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 61.12 MiB is free. Process 57915 has 15.83 GiB memory in use. Of the allocated memory 13.81 GiB is allocated by PyTorch, and 1.17 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"],"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 136.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 61.12 MiB is free. Process 57915 has 15.83 GiB memory in use. Of the allocated memory 13.81 GiB is allocated by PyTorch, and 1.17 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","output_type":"error"}],"execution_count":3}]}