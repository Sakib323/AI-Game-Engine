{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de5e4ebd-871f-49cb-aa32-d6b733649824",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting accelerate>=0.26.0\n",
      "  Downloading accelerate-1.12.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.11/site-packages (from accelerate>=0.26.0) (2.2.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from accelerate>=0.26.0) (24.2)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.11/site-packages (from accelerate>=0.26.0) (6.1.1)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.11/site-packages (from accelerate>=0.26.0) (6.0.2)\n",
      "Requirement already satisfied: torch>=2.0.0 in /opt/conda/lib/python3.11/site-packages (from accelerate>=0.26.0) (2.6.0+cu124)\n",
      "Requirement already satisfied: huggingface_hub>=0.21.0 in /opt/conda/lib/python3.11/site-packages (from accelerate>=0.26.0) (0.36.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/conda/lib/python3.11/site-packages (from accelerate>=0.26.0) (0.7.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from huggingface_hub>=0.21.0->accelerate>=0.26.0) (3.17.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.11/site-packages (from huggingface_hub>=0.21.0->accelerate>=0.26.0) (2024.12.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from huggingface_hub>=0.21.0->accelerate>=0.26.0) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.11/site-packages (from huggingface_hub>=0.21.0->accelerate>=0.26.0) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.11/site-packages (from huggingface_hub>=0.21.0->accelerate>=0.26.0) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/conda/lib/python3.11/site-packages (from huggingface_hub>=0.21.0->accelerate>=0.26.0) (1.2.0)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (3.1.5)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from sympy==1.13.1->torch>=2.0.0->accelerate>=0.26.0) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch>=2.0.0->accelerate>=0.26.0) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface_hub>=0.21.0->accelerate>=0.26.0) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface_hub>=0.21.0->accelerate>=0.26.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface_hub>=0.21.0->accelerate>=0.26.0) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface_hub>=0.21.0->accelerate>=0.26.0) (2024.12.14)\n",
      "Downloading accelerate-1.12.0-py3-none-any.whl (380 kB)\n",
      "Installing collected packages: accelerate\n",
      "Successfully installed accelerate-1.12.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U \"accelerate>=0.26.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6fd29d98-567f-4e2d-8800-c629897ee27c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting einops\n",
      "  Downloading einops-0.8.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting timm\n",
      "  Downloading timm-1.0.22-py3-none-any.whl.metadata (63 kB)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.11/site-packages (from timm) (2.6.0+cu124)\n",
      "Requirement already satisfied: torchvision in /opt/conda/lib/python3.11/site-packages (from timm) (0.21.0+cu124)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.11/site-packages (from timm) (6.0.2)\n",
      "Requirement already satisfied: huggingface_hub in /opt/conda/lib/python3.11/site-packages (from timm) (0.36.0)\n",
      "Requirement already satisfied: safetensors in /opt/conda/lib/python3.11/site-packages (from timm) (0.7.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from huggingface_hub->timm) (3.17.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.11/site-packages (from huggingface_hub->timm) (2024.12.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.11/site-packages (from huggingface_hub->timm) (24.2)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from huggingface_hub->timm) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.11/site-packages (from huggingface_hub->timm) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.11/site-packages (from huggingface_hub->timm) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/conda/lib/python3.11/site-packages (from huggingface_hub->timm) (1.2.0)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch->timm) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch->timm) (3.1.5)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch->timm) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch->timm) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch->timm) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /opt/conda/lib/python3.11/site-packages (from torch->timm) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /opt/conda/lib/python3.11/site-packages (from torch->timm) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /opt/conda/lib/python3.11/site-packages (from torch->timm) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /opt/conda/lib/python3.11/site-packages (from torch->timm) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /opt/conda/lib/python3.11/site-packages (from torch->timm) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /opt/conda/lib/python3.11/site-packages (from torch->timm) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /opt/conda/lib/python3.11/site-packages (from torch->timm) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /opt/conda/lib/python3.11/site-packages (from torch->timm) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch->timm) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch->timm) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /opt/conda/lib/python3.11/site-packages (from torch->timm) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/conda/lib/python3.11/site-packages (from torch->timm) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from sympy==1.13.1->torch->timm) (1.3.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.11/site-packages (from torchvision->timm) (2.2.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.11/site-packages (from torchvision->timm) (9.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch->timm) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface_hub->timm) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface_hub->timm) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface_hub->timm) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface_hub->timm) (2024.12.14)\n",
      "Downloading einops-0.8.1-py3-none-any.whl (64 kB)\n",
      "Downloading timm-1.0.22-py3-none-any.whl (2.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: einops, timm\n",
      "Successfully installed einops-0.8.1 timm-1.0.22\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mfatal: destination path 'AI-Game-Engine' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!pip install einops timm\n",
    "\n",
    "import sys, os\n",
    "!git clone https://github.com/Sakib323/AI-Game-Engine.git\n",
    "sys.path.append('/workspace/AI-Game-Engine') \n",
    "from mmfreelm.models.hgrn_bit.mesh_dit import MeshDiT_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "758745a5-af2e-420d-a72b-3bba2d33994d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda, dtype: torch.float32\n",
      "Loading tokenizer...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5666e18490f42579f490bcb72f24bfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6ccc30549b44ab9a953924cb950c919",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85ee61b4811e4a5294c3760d163a9487",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a73853d5ebb4b019cb2a112dfcc3dbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/437 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded.\n",
      "SUCCESS: MeshDiT models imported!\n"
     ]
    }
   ],
   "source": [
    "import os, json, shutil, logging, random, signal, time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.data import Dataset, Subset, RandomSampler, SequentialSampler, DataLoader\n",
    "from transformers import AutoTokenizer, Trainer, TrainingArguments\n",
    "from diffusers import DDIMScheduler\n",
    "from safetensors.torch import load_file as safetensors_load\n",
    "import wandb\n",
    "\n",
    "import torch._dynamo\n",
    "torch._dynamo.config.suppress_errors = True   \n",
    "torch._dynamo.config.verbose = False\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dtype = torch.float32\n",
    "print(f\"Using device: {device}, dtype: {dtype}\")              \n",
    "\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Sakib323/MMfreeLM-370M\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "print(\"Tokenizer loaded.\")\n",
    "\n",
    "print(\"SUCCESS: MeshDiT models imported!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9420115-2dc4-47e9-a133-2982f88941a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kagglehub\n",
      "  Downloading kagglehub-0.3.13-py3-none-any.whl.metadata (38 kB)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.11/site-packages (from kagglehub) (24.2)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.11/site-packages (from kagglehub) (6.0.2)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from kagglehub) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (from kagglehub) (4.67.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->kagglehub) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->kagglehub) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->kagglehub) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->kagglehub) (2024.12.14)\n",
      "Downloading kagglehub-0.3.13-py3-none-any.whl (68 kB)\n",
      "Installing collected packages: kagglehub\n",
      "Successfully installed kagglehub-0.3.13\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mDownloading from https://www.kaggle.com/api/v1/datasets/download/sakibahmed2022/meshdit-trained-model?dataset_version_number=6...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 887M/887M [00:07<00:00, 119MB/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/sakibahmed2022/meshdit-all-data-5-pt?dataset_version_number=1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 44.1G/44.1G [08:09<00:00, 96.6MB/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset downloaded to: /root/.cache/kagglehub/datasets/sakibahmed2022/meshdit-all-data-5-pt/versions/1\n",
      "Copying all_data.pt → /tmp/meshdit_cache/all_data.pt\n",
      "Copying all_data_1.pt → /tmp/meshdit_cache/all_data_1.pt\n",
      "Copying all_data_2.pt → /tmp/meshdit_cache/all_data_2.pt\n",
      "Copying all_data_3.pt → /tmp/meshdit_cache/all_data_3.pt\n",
      "Copying all_data_4.pt → /tmp/meshdit_cache/all_data_4.pt\n",
      "All .pt files ready in /tmp/meshdit_cache\n"
     ]
    }
   ],
   "source": [
    "!pip install kagglehub\n",
    "import kagglehub\n",
    "\n",
    "path = kagglehub.dataset_download(\"sakibahmed2022/meshdit-all-data-5-pt\")\n",
    "print(\"Dataset downloaded to:\", path)\n",
    "cache_dir = \"/tmp/meshdit_cache\"\n",
    "os.makedirs(cache_dir, exist_ok=True)\n",
    "\n",
    "file_names = [\"all_data.pt\", \"all_data_1.pt\", \"all_data_2.pt\", \"all_data_3.pt\", \"all_data_4.pt\"]\n",
    "local_paths = []\n",
    "\n",
    "for name in file_names:\n",
    "    src = os.path.join(path, name)\n",
    "    dst = os.path.join(cache_dir, name)\n",
    "    \n",
    "    if not os.path.exists(dst):\n",
    "        print(f\"Copying {name} → {dst}\")\n",
    "        shutil.copy2(src, dst)\n",
    "    else:\n",
    "        print(f\"Already cached: {dst}\")\n",
    "        \n",
    "    local_paths.append(dst)\n",
    "\n",
    "print(\"All .pt files ready in\", cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dcbbca08-09f0-4390-8e3f-9ed0facad6b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Freeing Disk Space ---\n",
      "Deleting original Kaggle cache at: /root/.cache/kagglehub/datasets/sakibahmed2022/meshdit-all-data-5-pt/versions/1\n",
      "✅ Redundant dataset deleted.\n",
      "\n",
      "DISK SPACE REMAINING: 49 GB\n",
      "All .pt files ready in /tmp/meshdit_cache\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Freeing Disk Space ---\")\n",
    "\n",
    "if os.path.exists(path):\n",
    "    print(f\"Deleting original Kaggle cache at: {path}\")\n",
    "    shutil.rmtree(path)\n",
    "    print(\"✅ Redundant dataset deleted.\")\n",
    "\n",
    "total, used, free = shutil.disk_usage(\"/\")\n",
    "print(f\"\\nDISK SPACE REMAINING: {free // (2**30)} GB\")\n",
    "print(\"All .pt files ready in\", cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e15a24b-4306-45a4-8e61-580945ff0b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    import os, random, shutil\n",
    "    import torch\n",
    "    import torch.nn.functional as F\n",
    "    from torch.utils.data import Dataset, Subset, RandomSampler, SequentialSampler, DataLoader\n",
    "    from transformers import Trainer, TrainingArguments, TrainerCallback \n",
    "    from diffusers import DDIMScheduler\n",
    "    from tqdm.notebook import tqdm\n",
    "    import wandb\n",
    "\n",
    "    import torch._dynamo\n",
    "    torch._dynamo.config.suppress_errors = True   \n",
    "    torch._dynamo.config.verbose = False\n",
    "\n",
    "    WANDB_TOKEN = \"89b06c10468af620747b4bd340f72fa5d56f6849\"\n",
    "    try:\n",
    "        wandb.login(key=WANDB_TOKEN)\n",
    "        os.environ[\"WANDB_PROJECT\"] = \"mesh-dit-3d-generation\"\n",
    "        use_wandb = True\n",
    "        print(\"Successfully logged in to W&B.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not log in to W&B: {e}. Disabling W&B.\")\n",
    "        use_wandb = False\n",
    "\n",
    "\n",
    "    class CustomDataCollator:\n",
    "        def __call__(self, features):\n",
    "            batch = {}\n",
    "            batch['x'] = torch.stack([f['x'] for f in features])\n",
    "            y_features = [f['y'] for f in features]\n",
    "            batch['y'] = {key: torch.stack([d[key] for d in y_features]) for key in y_features[0]}\n",
    "            return batch\n",
    "    \n",
    "    class MeshDiTTrainer(Trainer):\n",
    "        def __init__(self, *args, noise_scheduler, **kwargs):\n",
    "            super().__init__(*args, **kwargs)\n",
    "            self.noise_scheduler = noise_scheduler\n",
    "\n",
    "        def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "            x_start = inputs[\"x\"]\n",
    "            model_kwargs = inputs[\"y\"]\n",
    "            \n",
    "            noise = torch.randn_like(x_start)\n",
    "            batch_size = x_start.shape[0]\n",
    "            \n",
    "            # 2. Sample Timesteps\n",
    "            timesteps = torch.randint(0, self.noise_scheduler.config.num_train_timesteps, \n",
    "                                      (batch_size,), device=x_start.device).long()\n",
    "            noisy_latents = self.noise_scheduler.add_noise(x_start, noise, timesteps)\n",
    "            noise_pred = model(noisy_latents, timesteps, model_kwargs)\n",
    "            alphas = self.noise_scheduler.alphas_cumprod.to(x_start.device)\n",
    "            alpha_t = alphas[timesteps]\n",
    "            snr = alpha_t / (1 - alpha_t + 1e-8)\n",
    "            gamma = 5.0\n",
    "            weights = torch.clamp(snr, max=gamma) / snr\n",
    "            \n",
    "            loss_elementwise = F.mse_loss(noise_pred, noise, reduction=\"none\")\n",
    "            loss_per_sample = loss_elementwise.mean(dim=list(range(1, loss_elementwise.ndim)))\n",
    "            loss = (loss_per_sample * weights).mean()\n",
    "\n",
    "            return (loss, {\"loss\": loss}) if return_outputs else loss\n",
    "\n",
    "        def prediction_step(self, model, inputs, prediction_loss_only, ignore_keys=None):\n",
    "            inputs = self._prepare_inputs(inputs)\n",
    "            with torch.no_grad():\n",
    "                loss = self.compute_loss(model, inputs)\n",
    "            return (loss, None, None)\n",
    "\n",
    "\n",
    "    class ConcatenatedMeshDataset(Dataset):\n",
    "        def __init__(self, pt_paths):\n",
    "            self.pt_paths = pt_paths\n",
    "            self.cumsum = [0]\n",
    "            self.lengths = []\n",
    "            self.file_handles = []\n",
    "            for path in pt_paths:\n",
    "                print(f\"Opening {os.path.basename(path)} with mmap (zero copy)...\")\n",
    "                handle = torch.load(path, map_location='cpu', mmap=True)\n",
    "                data_list = handle['data']\n",
    "                length = len(data_list)\n",
    "                self.lengths.append(length)\n",
    "                self.cumsum.append(self.cumsum[-1] + length)\n",
    "                self.file_handles.append(data_list)\n",
    "            print(f\"Total samples: {sum(self.lengths):,} across {len(pt_paths)} files\")\n",
    "\n",
    "        def __len__(self):\n",
    "            return self.cumsum[-1]\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            for i, cum in enumerate(self.cumsum):\n",
    "                if idx < cum:\n",
    "                    file_idx = i - 1\n",
    "                    local_idx = idx - self.cumsum[file_idx]\n",
    "                    break\n",
    "            data_list = self.file_handles[file_idx]\n",
    "            sample = data_list[local_idx]\n",
    "            return {'x': sample['x'], 'y': sample['y']}\n",
    "\n",
    "    class EMACallback(TrainerCallback):\n",
    "        def __init__(self, decay=0.9999):\n",
    "            self.decay = decay\n",
    "            self.shadow = {}\n",
    "\n",
    "        def on_train_begin(self, args, state, control, model, **kwargs):\n",
    "            self.shadow = {n: p.clone().detach() for n, p in model.named_parameters() if p.requires_grad}\n",
    "            print(f\"EMA initialized with decay {self.decay}\")\n",
    "\n",
    "        def on_step_end(self, args, state, control, model, **kwargs):\n",
    "            with torch.no_grad():\n",
    "                for n, p in model.named_parameters():\n",
    "                    if n in self.shadow:\n",
    "                        self.shadow[n] = self.shadow[n].to(p.device) # Ensure device match\n",
    "                        self.shadow[n] = self.decay * self.shadow[n] + (1 - self.decay) * p.detach()\n",
    "\n",
    "        def on_save(self, args, state, control, **kwargs):\n",
    "            ema_path = os.path.join(args.output_dir, f\"checkpoint-{state.global_step}\", \"pytorch_model_ema.bin\")\n",
    "            os.makedirs(os.path.dirname(ema_path), exist_ok=True)\n",
    "            torch.save(self.shadow, ema_path)\n",
    "            print(f\"EMA weights saved to {ema_path}\")\n",
    "\n",
    "        def on_train_end(self, args, state, control, model, **kwargs):\n",
    "            print(\"Loading EMA weights into model for final save...\")\n",
    "            for n, p in model.named_parameters():\n",
    "                if n in self.shadow:\n",
    "                    p.data.copy_(self.shadow[n].to(p.device))\n",
    "\n",
    "    full_dataset = ConcatenatedMeshDataset(local_paths)\n",
    "    total_len = len(full_dataset)\n",
    "    eval_size = max(1, int(total_len * 0.10))\n",
    "    train_size = total_len - eval_size\n",
    "    train_subset = Subset(full_dataset, range(eval_size, total_len))\n",
    "    eval_subset = Subset(full_dataset, range(eval_size))\n",
    "\n",
    "    collator = CustomDataCollator()\n",
    "    scheduler = DDIMScheduler(num_train_timesteps=2000, beta_schedule=\"linear\", prediction_type=\"epsilon\")\n",
    "\n",
    "    \n",
    "    generator = torch.Generator().manual_seed(42)\n",
    "    train_sampler = RandomSampler(train_subset, generator=torch.Generator().manual_seed(42))\n",
    "    eval_sampler = SequentialSampler(eval_subset)\n",
    "    \n",
    "    print(f\"Train: {train_size:,} | Eval: {eval_size:,}\")\n",
    "\n",
    "\n",
    "\n",
    "    print(\"\\n\" + \"=\"*6060)\n",
    "    print(\"PHASE 1: Training from scratch (FAST!)\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    model_p1 = MeshDiT_models['MeshDiT-S'](\n",
    "        input_tokens=2048,\n",
    "        vocab_size=tokenizer.vocab_size,\n",
    "        use_rope=True,\n",
    "        use_ternary_rope=True,\n",
    "        image_condition=False,\n",
    "        full_precision=True,\n",
    "        optimized_bitlinear=False,\n",
    "    ).to(device, dtype=dtype)\n",
    "\n",
    "    model_path = kagglehub.dataset_download(\"sakibahmed2022/meshdit-trained-model\")    \n",
    "    model_path=os.path.join(model_path, \"model.safetensors\")\n",
    "    state_dict_p1 = safetensors_load(model_path, device=\"cpu\")\n",
    "    model_p1.load_state_dict(state_dict_p1)\n",
    "    print(\"✅ Phase 1 weights loaded for fine-tuning\")\n",
    "    \n",
    "\n",
    "    print(model_p1)\n",
    "    \n",
    "    args_p1 = TrainingArguments(\n",
    "        output_dir=\"./phase1_ckpt\",\n",
    "        num_train_epochs=10,\n",
    "        per_device_train_batch_size=64, \n",
    "        gradient_accumulation_steps=8,\n",
    "        \n",
    "        learning_rate=2e-5,          \n",
    "        max_grad_norm=1.0,           \n",
    "        weight_decay=0.02,           \n",
    "        warmup_ratio=0.05,\n",
    "        \n",
    "        lr_scheduler_type=\"cosine\",  \n",
    "        logging_steps=50,\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps=500,\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=1000,\n",
    "        save_total_limit=2,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\", \n",
    "        greater_is_better=False,\n",
    "        fp16=True,\n",
    "        dataloader_num_workers=4,\n",
    "        remove_unused_columns=False,\n",
    "        report_to=\"wandb\" if use_wandb else \"tensorboard\",\n",
    "        run_name=\"MeshDiT-S-Optimized\",\n",
    "        torch_compile=True,        \n",
    "        torch_compile_backend=\"inductor\",\n",
    "    )\n",
    "    \n",
    "    trainer_p1 = MeshDiTTrainer(\n",
    "        model=model_p1,\n",
    "        args=args_p1,\n",
    "        train_dataset=train_subset,\n",
    "        eval_dataset=eval_subset,\n",
    "        data_collator=collator,\n",
    "        noise_scheduler=scheduler,\n",
    "    )\n",
    "    \n",
    "    trainer_p1.train_dataloader = DataLoader(\n",
    "        train_subset,\n",
    "        batch_size=args_p1.per_device_train_batch_size,\n",
    "        sampler=train_sampler,\n",
    "        collate_fn=collator,\n",
    "        num_workers=4,\n",
    "        pin_memory=True,\n",
    "        prefetch_factor=4,\n",
    "        persistent_workers=True\n",
    "    )\n",
    "    \n",
    "    trainer_p1.eval_dataloader = DataLoader(\n",
    "        eval_subset,\n",
    "        batch_size=args_p1.per_device_train_batch_size,\n",
    "        sampler=eval_sampler,\n",
    "        collate_fn=collator,\n",
    "        num_workers=4,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    trainer_p1.add_callback(EMACallback(decay=0.9999))\n",
    "\n",
    "    trainer_p1.train()\n",
    "    \n",
    "    trainer_p1.save_model(\"./mesh_dit_final\")\n",
    "    tokenizer.save_pretrained(\"./mesh_dit_final\")\n",
    "    \n",
    "    if use_wandb:\n",
    "        wandb.finish()\n",
    "        \n",
    "    print(\"\\nTRAINING COMPLETE!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb733664-3ede-4aea-856a-91223fcd5b0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully logged in to W&B.\n",
      "Opening all_data.pt with mmap (zero copy)...\n",
      "Opening all_data_1.pt with mmap (zero copy)...\n",
      "Opening all_data_2.pt with mmap (zero copy)...\n",
      "Opening all_data_3.pt with mmap (zero copy)...\n",
      "Opening all_data_4.pt with mmap (zero copy)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mmfreelm.models.hgrn_bit.mesh_dit:Absolute positional embeddings are disabled for this model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 97,308 across 5 files\n",
      "Train: 87,578 | Eval: 9,730\n",
      "\n",
      "============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================\n",
      "PHASE 1: Training from scratch (FAST!)\n",
      "============================================================\n",
      "Initializing RotaryEmbedding with theta=10000.0 and ternary=True\n",
      "\n",
      "[RotaryEmbedding] Initialized with: dim=64, max_pos=2048, base=10000.0, ternary=True\n",
      "\n",
      "Initializing RotaryEmbedding with theta=10000.0 and ternary=True\n",
      "\n",
      "[RotaryEmbedding] Initialized with: dim=64, max_pos=2048, base=10000.0, ternary=True\n",
      "\n",
      "Initializing RotaryEmbedding with theta=10000.0 and ternary=True\n",
      "\n",
      "[RotaryEmbedding] Initialized with: dim=64, max_pos=2048, base=10000.0, ternary=True\n",
      "\n",
      "Initializing RotaryEmbedding with theta=10000.0 and ternary=True\n",
      "\n",
      "[RotaryEmbedding] Initialized with: dim=64, max_pos=2048, base=10000.0, ternary=True\n",
      "\n",
      "Initializing RotaryEmbedding with theta=10000.0 and ternary=True\n",
      "\n",
      "[RotaryEmbedding] Initialized with: dim=64, max_pos=2048, base=10000.0, ternary=True\n",
      "\n",
      "Initializing RotaryEmbedding with theta=10000.0 and ternary=True\n",
      "\n",
      "[RotaryEmbedding] Initialized with: dim=64, max_pos=2048, base=10000.0, ternary=True\n",
      "\n",
      "Initializing RotaryEmbedding with theta=10000.0 and ternary=True\n",
      "\n",
      "[RotaryEmbedding] Initialized with: dim=64, max_pos=2048, base=10000.0, ternary=True\n",
      "\n",
      "Initializing RotaryEmbedding with theta=10000.0 and ternary=True\n",
      "\n",
      "[RotaryEmbedding] Initialized with: dim=64, max_pos=2048, base=10000.0, ternary=True\n",
      "\n",
      "Initializing RotaryEmbedding with theta=10000.0 and ternary=True\n",
      "\n",
      "[RotaryEmbedding] Initialized with: dim=64, max_pos=2048, base=10000.0, ternary=True\n",
      "\n",
      "Initializing RotaryEmbedding with theta=10000.0 and ternary=True\n",
      "\n",
      "[RotaryEmbedding] Initialized with: dim=64, max_pos=2048, base=10000.0, ternary=True\n",
      "\n",
      "Initializing RotaryEmbedding with theta=10000.0 and ternary=True\n",
      "\n",
      "[RotaryEmbedding] Initialized with: dim=64, max_pos=2048, base=10000.0, ternary=True\n",
      "\n",
      "Initializing RotaryEmbedding with theta=10000.0 and ternary=True\n",
      "\n",
      "[RotaryEmbedding] Initialized with: dim=64, max_pos=2048, base=10000.0, ternary=True\n",
      "\n",
      "Initializing RotaryEmbedding with theta=10000.0 and ternary=True\n",
      "\n",
      "[RotaryEmbedding] Initialized with: dim=64, max_pos=2048, base=10000.0, ternary=True\n",
      "\n",
      "Initializing RotaryEmbedding with theta=10000.0 and ternary=True\n",
      "\n",
      "[RotaryEmbedding] Initialized with: dim=64, max_pos=2048, base=10000.0, ternary=True\n",
      "\n",
      "Initializing RotaryEmbedding with theta=10000.0 and ternary=True\n",
      "\n",
      "[RotaryEmbedding] Initialized with: dim=64, max_pos=2048, base=10000.0, ternary=True\n",
      "\n",
      "Initializing RotaryEmbedding with theta=10000.0 and ternary=True\n",
      "\n",
      "[RotaryEmbedding] Initialized with: dim=64, max_pos=2048, base=10000.0, ternary=True\n",
      "\n",
      "Initializing RotaryEmbedding with theta=10000.0 and ternary=True\n",
      "\n",
      "[RotaryEmbedding] Initialized with: dim=64, max_pos=2048, base=10000.0, ternary=True\n",
      "\n",
      "Initializing RotaryEmbedding with theta=10000.0 and ternary=True\n",
      "\n",
      "[RotaryEmbedding] Initialized with: dim=64, max_pos=2048, base=10000.0, ternary=True\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The speedups for torchdynamo mostly come with GPU Ampere or higher and which is not detected here.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Phase 1 weights loaded for fine-tuning\n",
      "MeshDiT(\n",
      "  (x_embedder): Linear(in_features=64, out_features=384, bias=True)\n",
      "  (t_embedder): TimestepEmbedder(\n",
      "    (mlp): Sequential(\n",
      "      (0): Linear(in_features=256, out_features=384, bias=True)\n",
      "      (1): SiLU()\n",
      "      (2): Linear(in_features=384, out_features=384, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (y_embedder): TextEmbedder(\n",
      "    (embedding): Embedding(32001, 384)\n",
      "    (mlp): FullPrecisionMLP(\n",
      "      (gate_proj): Linear(in_features=384, out_features=2048, bias=False)\n",
      "      (down_proj): Linear(in_features=1024, out_features=384, bias=False)\n",
      "      (act_fn): SiLU()\n",
      "    )\n",
      "  )\n",
      "  (image_embedder): ImageLatentEmbedder(\n",
      "    (mlp): Sequential(\n",
      "      (0): Linear(in_features=16384, out_features=384, bias=True)\n",
      "      (1): SiLU()\n",
      "      (2): Linear(in_features=384, out_features=384, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (dual_stream_blocks): ModuleList(\n",
      "    (0-5): 6 x DualStreamBlock(\n",
      "      (norm1): LayerNorm(384, elementwise_affine=False, eps=1e-06)\n",
      "      (norm2): LayerNorm(384, elementwise_affine=False, eps=1e-06)\n",
      "      (norm3): LayerNorm(384, elementwise_affine=False, eps=1e-06)\n",
      "      (norm4): LayerNorm(384, elementwise_affine=False, eps=1e-06)\n",
      "      (attn_x): HGRNBitAttention(\n",
      "        (i_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      "        (f_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      "        (g_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      "        (g_norm): FusedRMSNormSwishGate()\n",
      "        (o_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      "        (rotary_emb): RotaryEmbedding()\n",
      "      )\n",
      "      (attn_c): HGRNBitAttention(\n",
      "        (i_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      "        (f_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      "        (g_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      "        (g_norm): FusedRMSNormSwishGate()\n",
      "        (o_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      "        (rotary_emb): RotaryEmbedding()\n",
      "      )\n",
      "      (dual_cross_attn): DualCrossAttention(\n",
      "        (to_q_x): Linear(in_features=384, out_features=384, bias=False)\n",
      "        (to_kv_c): Linear(in_features=384, out_features=768, bias=False)\n",
      "        (lora_A_x): Linear(in_features=384, out_features=16, bias=False)\n",
      "        (lora_B_x): Linear(in_features=16, out_features=384, bias=False)\n",
      "        (proj_x): Linear(in_features=384, out_features=384, bias=False)\n",
      "        (to_q_c): Linear(in_features=384, out_features=384, bias=False)\n",
      "        (to_kv_x): Linear(in_features=384, out_features=768, bias=False)\n",
      "        (lora_A_c): Linear(in_features=384, out_features=16, bias=False)\n",
      "        (lora_B_c): Linear(in_features=16, out_features=384, bias=False)\n",
      "        (proj_c): Linear(in_features=384, out_features=384, bias=False)\n",
      "      )\n",
      "      (mlp): HGRNBitMLP(\n",
      "        (gate_proj): Linear(in_features=384, out_features=2048, bias=False)\n",
      "        (down_proj): Linear(in_features=1024, out_features=384, bias=False)\n",
      "        (act_fn): SiLU()\n",
      "      )\n",
      "      (mlp_c): HGRNBitMLP(\n",
      "        (gate_proj): Linear(in_features=384, out_features=2048, bias=False)\n",
      "        (down_proj): Linear(in_features=1024, out_features=384, bias=False)\n",
      "        (act_fn): SiLU()\n",
      "      )\n",
      "      (adaLN_modulation): FullPrecisionAdaLNConditioning(\n",
      "        (input_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      "        (mlp): FullPrecisionMLP(\n",
      "          (gate_proj): Linear(in_features=384, out_features=2048, bias=False)\n",
      "          (down_proj): Linear(in_features=1024, out_features=384, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (output_proj): Linear(in_features=384, out_features=4608, bias=True)\n",
      "        (norm): RMSNorm(4608, eps=1e-06)\n",
      "        (out_proj): Linear(in_features=4608, out_features=4608, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (single_stream_blocks): ModuleList(\n",
      "    (0-5): 6 x SingleStreamBlock(\n",
      "      (norm1): LayerNorm(384, elementwise_affine=False, eps=1e-06)\n",
      "      (norm2): LayerNorm(384, elementwise_affine=False, eps=1e-06)\n",
      "      (attn): HGRNBitAttention(\n",
      "        (i_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      "        (f_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      "        (g_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      "        (g_norm): FusedRMSNormSwishGate()\n",
      "        (o_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      "        (rotary_emb): RotaryEmbedding()\n",
      "      )\n",
      "      (mlp): HGRNBitMLP(\n",
      "        (gate_proj): Linear(in_features=384, out_features=2048, bias=False)\n",
      "        (down_proj): Linear(in_features=1024, out_features=384, bias=False)\n",
      "        (act_fn): SiLU()\n",
      "      )\n",
      "      (adaLN_modulation): FullPrecisionAdaLNConditioning(\n",
      "        (input_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      "        (mlp): FullPrecisionMLP(\n",
      "          (gate_proj): Linear(in_features=384, out_features=2048, bias=False)\n",
      "          (down_proj): Linear(in_features=1024, out_features=384, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (output_proj): Linear(in_features=384, out_features=2304, bias=True)\n",
      "        (norm): RMSNorm(2304, eps=1e-06)\n",
      "        (out_proj): Linear(in_features=2304, out_features=2304, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (final_layer): FinalLayer(\n",
      "    (norm_final): LayerNorm(384, elementwise_affine=False, eps=1e-06)\n",
      "    (linear): Linear(in_features=384, out_features=64, bias=True)\n",
      "    (adaLN_modulation): FullPrecisionAdaLNConditioning(\n",
      "      (input_proj): Linear(in_features=384, out_features=384, bias=False)\n",
      "      (mlp): FullPrecisionMLP(\n",
      "        (gate_proj): Linear(in_features=384, out_features=2048, bias=False)\n",
      "        (down_proj): Linear(in_features=1024, out_features=384, bias=False)\n",
      "        (act_fn): SiLU()\n",
      "      )\n",
      "      (output_proj): Linear(in_features=384, out_features=768, bias=True)\n",
      "      (norm): RMSNorm(768, eps=1e-06)\n",
      "      (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/wandb/run-20251127_191538-jnz4gkw8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/sakibahmed2018go/mesh-dit-3d-generation/runs/jnz4gkw8' target=\"_blank\">MeshDiT-S-Optimized</a></strong> to <a href='https://wandb.ai/sakibahmed2018go/mesh-dit-3d-generation' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/sakibahmed2018go/mesh-dit-3d-generation' target=\"_blank\">https://wandb.ai/sakibahmed2018go/mesh-dit-3d-generation</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/sakibahmed2018go/mesh-dit-3d-generation/runs/jnz4gkw8' target=\"_blank\">https://wandb.ai/sakibahmed2018go/mesh-dit-3d-generation/runs/jnz4gkw8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EMA initialized with decay 0.9999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233] WON'T CONVERT torch_dynamo_resume_in_forward_at_136 /workspace/AI-Game-Engine/mmfreelm/layers/hgrn_bit.py line 136 \n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233] due to: \n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233] Traceback (most recent call last):\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]   File \"/opt/conda/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py\", line 1164, in __call__\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]     result = self._inner_convert(\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]              ^^^^^^^^^^^^^^^^^^^^\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]   File \"/opt/conda/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py\", line 547, in __call__\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]     return _compile(\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]            ^^^^^^^^^\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]   File \"/opt/conda/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py\", line 1036, in _compile\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]     raise InternalTorchDynamoError(\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]   File \"/opt/conda/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py\", line 986, in _compile\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]   File \"/opt/conda/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py\", line 715, in compile_inner\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]     return _compile_inner(code, one_graph, hooks, transform)\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]   File \"/opt/conda/lib/python3.11/site-packages/torch/_utils_internal.py\", line 95, in wrapper_function\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]     return function(*args, **kwargs)\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]   File \"/opt/conda/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py\", line 750, in _compile_inner\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]     out_code = transform_code_object(code, transform)\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]   File \"/opt/conda/lib/python3.11/site-packages/torch/_dynamo/bytecode_transformation.py\", line 1361, in transform_code_object\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]     transformations(instructions, code_options)\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]   File \"/opt/conda/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py\", line 231, in _fn\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]     return fn(*args, **kwargs)\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]            ^^^^^^^^^^^^^^^^^^^\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]   File \"/opt/conda/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py\", line 662, in transform\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]     tracer.run()\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]   File \"/opt/conda/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py\", line 2868, in run\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]     super().run()\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]   File \"/opt/conda/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py\", line 1052, in run\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]     while self.step():\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]           ^^^^^^^^^^^\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]   File \"/opt/conda/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py\", line 962, in step\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]     self.dispatch_table[inst.opcode](self, inst)\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]   File \"/opt/conda/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py\", line 659, in wrapper\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]     return inner_fn(self, inst)\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]            ^^^^^^^^^^^^^^^^^^^^\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]   File \"/opt/conda/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py\", line 2341, in CALL\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]     self._call(inst)\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]   File \"/opt/conda/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py\", line 2335, in _call\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]     self.call_function(fn, args, kwargs)\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]   File \"/opt/conda/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py\", line 897, in call_function\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]     self.push(fn.call_function(self, args, kwargs))  # type: ignore[arg-type]\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]   File \"/opt/conda/lib/python3.11/site-packages/torch/_dynamo/variables/functions.py\", line 317, in call_function\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]     return super().call_function(tx, args, kwargs)\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]   File \"/opt/conda/lib/python3.11/site-packages/torch/_dynamo/variables/functions.py\", line 118, in call_function\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]     return tx.inline_user_function_return(self, [*self.self_args(), *args], kwargs)\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]   File \"/opt/conda/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py\", line 903, in inline_user_function_return\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]     return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]   File \"/opt/conda/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py\", line 3072, in inline_call\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]     return cls.inline_call_(parent, func, args, kwargs)\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]   File \"/opt/conda/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py\", line 3198, in inline_call_\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]     tracer.run()\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]   File \"/opt/conda/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py\", line 1052, in run\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]     while self.step():\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]           ^^^^^^^^^^^\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]   File \"/opt/conda/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py\", line 962, in step\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]     self.dispatch_table[inst.opcode](self, inst)\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]   File \"/opt/conda/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py\", line 659, in wrapper\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]     return inner_fn(self, inst)\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]            ^^^^^^^^^^^^^^^^^^^^\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]   File \"/opt/conda/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py\", line 2341, in CALL\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]     self._call(inst)\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]   File \"/opt/conda/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py\", line 2335, in _call\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]     self.call_function(fn, args, kwargs)\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]   File \"/opt/conda/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py\", line 897, in call_function\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]     self.push(fn.call_function(self, args, kwargs))  # type: ignore[arg-type]\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]   File \"/opt/conda/lib/python3.11/site-packages/torch/_dynamo/variables/misc.py\", line 1022, in call_function\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]     return self.obj.call_method(tx, self.name, args, kwargs)\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]   File \"/opt/conda/lib/python3.11/site-packages/torch/_dynamo/variables/misc.py\", line 759, in call_method\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]     return self.call_apply(tx, args, kwargs)\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]   File \"/opt/conda/lib/python3.11/site-packages/torch/_dynamo/variables/misc.py\", line 681, in call_apply\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]     ).call_function(tx, args, kwargs)\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]   File \"/opt/conda/lib/python3.11/site-packages/torch/_dynamo/variables/higher_order_ops.py\", line 2531, in call_function\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]     new_fwd_graph_outputs = pytree.tree_map(lambda x: x.node, new_fwd_graph_outputs)\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]   File \"/opt/conda/lib/python3.11/site-packages/torch/utils/_pytree.py\", line 991, in tree_map\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]     return treespec.unflatten(map(func, *flat_args))\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]   File \"/opt/conda/lib/python3.11/site-packages/torch/utils/_pytree.py\", line 830, in unflatten\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]     leaves = list(leaves)\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]              ^^^^^^^^^^^^\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]   File \"/opt/conda/lib/python3.11/site-packages/torch/_dynamo/variables/higher_order_ops.py\", line 2531, in <lambda>\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]     new_fwd_graph_outputs = pytree.tree_map(lambda x: x.node, new_fwd_graph_outputs)\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]                                                       ^^^^^^\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233] torch._dynamo.exc.InternalTorchDynamoError: AttributeError: 'NoneType' object has no attribute 'node'\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233] \n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233] from user code:\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]    File \"/workspace/AI-Game-Engine/mmfreelm/layers/hgrn_bit.py\", line 152, in torch_dynamo_resume_in_forward_at_136\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]     o, recurrent_state = fused_recurrent_hgrn(i, f, initial_state=recurrent_state, output_final_state=use_cache)\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]   File \"/workspace/AI-Game-Engine/mmfreelm/ops/hgrn/recurrent_fuse.py\", line 184, in fused_recurrent_hgrn\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]     o, final_state = FusedRecurrentHGRNFunction.apply(x, g, initial_state, output_final_state)\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233] \n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233] \n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233] Traceback (most recent call last):\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]   File \"/opt/conda/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py\", line 1164, in __call__\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]     result = self._inner_convert(\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]              ^^^^^^^^^^^^^^^^^^^^\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]   File \"/opt/conda/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py\", line 547, in __call__\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]     return _compile(\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]            ^^^^^^^^^\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]   File \"/opt/conda/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py\", line 1036, in _compile\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]     raise InternalTorchDynamoError(\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]   File \"/opt/conda/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py\", line 986, in _compile\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]   File \"/opt/conda/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py\", line 715, in compile_inner\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]     return _compile_inner(code, one_graph, hooks, transform)\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]   File \"/opt/conda/lib/python3.11/site-packages/torch/_utils_internal.py\", line 95, in wrapper_function\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]     return function(*args, **kwargs)\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]   File \"/opt/conda/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py\", line 750, in _compile_inner\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]     out_code = transform_code_object(code, transform)\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]   File \"/opt/conda/lib/python3.11/site-packages/torch/_dynamo/bytecode_transformation.py\", line 1361, in transform_code_object\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]     transformations(instructions, code_options)\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]   File \"/opt/conda/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py\", line 231, in _fn\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]     return fn(*args, **kwargs)\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]            ^^^^^^^^^^^^^^^^^^^\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]   File \"/opt/conda/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py\", line 662, in transform\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]     tracer.run()\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]   File \"/opt/conda/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py\", line 2868, in run\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]     super().run()\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]   File \"/opt/conda/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py\", line 1052, in run\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]     while self.step():\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]           ^^^^^^^^^^^\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]   File \"/opt/conda/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py\", line 962, in step\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]     self.dispatch_table[inst.opcode](self, inst)\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]   File \"/opt/conda/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py\", line 659, in wrapper\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]     return inner_fn(self, inst)\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]            ^^^^^^^^^^^^^^^^^^^^\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]   File \"/opt/conda/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py\", line 2341, in CALL\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]     self._call(inst)\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]   File \"/opt/conda/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py\", line 2335, in _call\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]     self.call_function(fn, args, kwargs)\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]   File \"/opt/conda/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py\", line 897, in call_function\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]     self.push(fn.call_function(self, args, kwargs))  # type: ignore[arg-type]\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]   File \"/opt/conda/lib/python3.11/site-packages/torch/_dynamo/variables/functions.py\", line 317, in call_function\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]     return super().call_function(tx, args, kwargs)\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]   File \"/opt/conda/lib/python3.11/site-packages/torch/_dynamo/variables/functions.py\", line 118, in call_function\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]     return tx.inline_user_function_return(self, [*self.self_args(), *args], kwargs)\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]   File \"/opt/conda/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py\", line 903, in inline_user_function_return\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]     return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]   File \"/opt/conda/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py\", line 3072, in inline_call\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]     return cls.inline_call_(parent, func, args, kwargs)\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]   File \"/opt/conda/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py\", line 3198, in inline_call_\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]     tracer.run()\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]   File \"/opt/conda/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py\", line 1052, in run\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]     while self.step():\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]           ^^^^^^^^^^^\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]   File \"/opt/conda/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py\", line 962, in step\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]     self.dispatch_table[inst.opcode](self, inst)\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]   File \"/opt/conda/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py\", line 659, in wrapper\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]     return inner_fn(self, inst)\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]            ^^^^^^^^^^^^^^^^^^^^\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]   File \"/opt/conda/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py\", line 2341, in CALL\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]     self._call(inst)\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]   File \"/opt/conda/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py\", line 2335, in _call\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]     self.call_function(fn, args, kwargs)\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]   File \"/opt/conda/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py\", line 897, in call_function\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]     self.push(fn.call_function(self, args, kwargs))  # type: ignore[arg-type]\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]   File \"/opt/conda/lib/python3.11/site-packages/torch/_dynamo/variables/misc.py\", line 1022, in call_function\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]     return self.obj.call_method(tx, self.name, args, kwargs)\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]   File \"/opt/conda/lib/python3.11/site-packages/torch/_dynamo/variables/misc.py\", line 759, in call_method\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]     return self.call_apply(tx, args, kwargs)\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]   File \"/opt/conda/lib/python3.11/site-packages/torch/_dynamo/variables/misc.py\", line 681, in call_apply\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]     ).call_function(tx, args, kwargs)\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]   File \"/opt/conda/lib/python3.11/site-packages/torch/_dynamo/variables/higher_order_ops.py\", line 2531, in call_function\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]     new_fwd_graph_outputs = pytree.tree_map(lambda x: x.node, new_fwd_graph_outputs)\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]   File \"/opt/conda/lib/python3.11/site-packages/torch/utils/_pytree.py\", line 991, in tree_map\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]     return treespec.unflatten(map(func, *flat_args))\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]   File \"/opt/conda/lib/python3.11/site-packages/torch/utils/_pytree.py\", line 830, in unflatten\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]     leaves = list(leaves)\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]              ^^^^^^^^^^^^\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]   File \"/opt/conda/lib/python3.11/site-packages/torch/_dynamo/variables/higher_order_ops.py\", line 2531, in <lambda>\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]     new_fwd_graph_outputs = pytree.tree_map(lambda x: x.node, new_fwd_graph_outputs)\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]                                                       ^^^^^^\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233] torch._dynamo.exc.InternalTorchDynamoError: AttributeError: 'NoneType' object has no attribute 'node'\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233] \n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233] from user code:\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]    File \"/workspace/AI-Game-Engine/mmfreelm/layers/hgrn_bit.py\", line 152, in torch_dynamo_resume_in_forward_at_136\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]     o, recurrent_state = fused_recurrent_hgrn(i, f, initial_state=recurrent_state, output_final_state=use_cache)\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]   File \"/workspace/AI-Game-Engine/mmfreelm/ops/hgrn/recurrent_fuse.py\", line 184, in fused_recurrent_hgrn\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233]     o, final_state = FusedRecurrentHGRNFunction.apply(x, g, initial_state, output_final_state)\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233] \n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
      "W1127 19:16:01.374000 969 site-packages/torch/_dynamo/convert_frame.py:1233] \n",
      "W1127 19:16:03.160000 969 site-packages/torch/_dynamo/convert_frame.py:1233] WON'T CONVERT fused_recurrent_hgrn /workspace/AI-Game-Engine/mmfreelm/ops/hgrn/recurrent_fuse.py line 176 \n",
      "W1127 19:16:03.160000 969 site-packages/torch/_dynamo/convert_frame.py:1233] due to: \n",
      "W1127 19:16:03.160000 969 site-packages/torch/_dynamo/convert_frame.py:1233] Traceback (most recent call last):\n",
      "W1127 19:16:03.160000 969 site-packages/torch/_dynamo/convert_frame.py:1233]   File \"/opt/conda/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py\", line 1164, in __call__\n",
      "W1127 19:16:03.160000 969 site-packages/torch/_dynamo/convert_frame.py:1233]     result = self._inner_convert(\n",
      "W1127 19:16:03.160000 969 site-packages/torch/_dynamo/convert_frame.py:1233]              ^^^^^^^^^^^^^^^^^^^^\n",
      "W1127 19:16:03.160000 969 site-packages/torch/_dynamo/convert_frame.py:1233]   File \"/opt/conda/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py\", line 547, in __call__\n",
      "W1127 19:16:03.160000 969 site-packages/torch/_dynamo/convert_frame.py:1233]     return _compile(\n",
      "W1127 19:16:03.160000 969 site-packages/torch/_dynamo/convert_frame.py:1233]            ^^^^^^^^^\n",
      "W1127 19:16:03.160000 969 site-packages/torch/_dynamo/convert_frame.py:1233]   File \"/opt/conda/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py\", line 1036, in _compile\n",
      "W1127 19:16:03.160000 969 site-packages/torch/_dynamo/convert_frame.py:1233]     raise InternalTorchDynamoError(\n",
      "W1127 19:16:03.160000 969 site-packages/torch/_dynamo/convert_frame.py:1233]   File \"/opt/conda/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py\", line 986, in _compile\n",
      "W1127 19:16:03.160000 969 site-packages/torch/_dynamo/convert_frame.py:1233]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
      "W1127 19:16:03.160000 969 site-packages/torch/_dynamo/convert_frame.py:1233]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W1127 19:16:03.160000 969 site-packages/torch/_dynamo/convert_frame.py:1233]   File \"/opt/conda/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py\", line 715, in compile_inner\n",
      "W1127 19:16:03.160000 969 site-packages/torch/_dynamo/convert_frame.py:1233]     return _compile_inner(code, one_graph, hooks, transform)\n",
      "W1127 19:16:03.160000 969 site-packages/torch/_dynamo/convert_frame.py:1233]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W1127 19:16:03.160000 969 site-packages/torch/_dynamo/convert_frame.py:1233]   File \"/opt/conda/lib/python3.11/site-packages/torch/_utils_internal.py\", line 95, in wrapper_function\n",
      "W1127 19:16:03.160000 969 site-packages/torch/_dynamo/convert_frame.py:1233]     return function(*args, **kwargs)\n",
      "W1127 19:16:03.160000 969 site-packages/torch/_dynamo/convert_frame.py:1233]            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W1127 19:16:03.160000 969 site-packages/torch/_dynamo/convert_frame.py:1233]   File \"/opt/conda/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py\", line 750, in _compile_inner\n",
      "W1127 19:16:03.160000 969 site-packages/torch/_dynamo/convert_frame.py:1233]     out_code = transform_code_object(code, transform)\n",
      "W1127 19:16:03.160000 969 site-packages/torch/_dynamo/convert_frame.py:1233]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W1127 19:16:03.160000 969 site-packages/torch/_dynamo/convert_frame.py:1233]   File \"/opt/conda/lib/python3.11/site-packages/torch/_dynamo/bytecode_transformation.py\", line 1361, in transform_code_object\n",
      "W1127 19:16:03.160000 969 site-packages/torch/_dynamo/convert_frame.py:1233]     transformations(instructions, code_options)\n",
      "W1127 19:16:03.160000 969 site-packages/torch/_dynamo/convert_frame.py:1233]   File \"/opt/conda/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py\", line 231, in _fn\n",
      "W1127 19:16:03.160000 969 site-packages/torch/_dynamo/convert_frame.py:1233]     return fn(*args, **kwargs)\n",
      "W1127 19:16:03.160000 969 site-packages/torch/_dynamo/convert_frame.py:1233]            ^^^^^^^^^^^^^^^^^^^\n",
      "W1127 19:16:03.160000 969 site-packages/torch/_dynamo/convert_frame.py:1233]   File \"/opt/conda/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py\", line 662, in transform\n",
      "W1127 19:16:03.160000 969 site-packages/torch/_dynamo/convert_frame.py:1233]     tracer.run()\n",
      "W1127 19:16:03.160000 969 site-packages/torch/_dynamo/convert_frame.py:1233]   File \"/opt/conda/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py\", line 2868, in run\n",
      "W1127 19:16:03.160000 969 site-packages/torch/_dynamo/convert_frame.py:1233]     super().run()\n",
      "W1127 19:16:03.160000 969 site-packages/torch/_dynamo/convert_frame.py:1233]   File \"/opt/conda/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py\", line 1052, in run\n",
      "W1127 19:16:03.160000 969 site-packages/torch/_dynamo/convert_frame.py:1233]     while self.step():\n",
      "W1127 19:16:03.160000 969 site-packages/torch/_dynamo/convert_frame.py:1233]           ^^^^^^^^^^^\n",
      "W1127 19:16:03.160000 969 site-packages/torch/_dynamo/convert_frame.py:1233]   File \"/opt/conda/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py\", line 962, in step\n",
      "W1127 19:16:03.160000 969 site-packages/torch/_dynamo/convert_frame.py:1233]     self.dispatch_table[inst.opcode](self, inst)\n",
      "W1127 19:16:03.160000 969 site-packages/torch/_dynamo/convert_frame.py:1233]   File \"/opt/conda/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py\", line 659, in wrapper\n",
      "W1127 19:16:03.160000 969 site-packages/torch/_dynamo/convert_frame.py:1233]     return inner_fn(self, inst)\n",
      "W1127 19:16:03.160000 969 site-packages/torch/_dynamo/convert_frame.py:1233]            ^^^^^^^^^^^^^^^^^^^^\n",
      "W1127 19:16:03.160000 969 site-packages/torch/_dynamo/convert_frame.py:1233]   File \"/opt/conda/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py\", line 2341, in CALL\n",
      "W1127 19:16:03.160000 969 site-packages/torch/_dynamo/convert_frame.py:1233]     self._call(inst)\n",
      "W1127 19:16:03.160000 969 site-packages/torch/_dynamo/convert_frame.py:1233]   File \"/opt/conda/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py\", line 2335, in _call\n",
      "W1127 19:16:03.160000 969 site-packages/torch/_dynamo/convert_frame.py:1233]     self.call_function(fn, args, kwargs)\n",
      "W1127 19:16:03.160000 969 site-packages/torch/_dynamo/convert_frame.py:1233]   File \"/opt/conda/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py\", line 897, in call_function\n",
      "W1127 19:16:03.160000 969 site-packages/torch/_dynamo/convert_frame.py:1233]     self.push(fn.call_function(self, args, kwargs))  # type: ignore[arg-type]\n",
      "W1127 19:16:03.160000 969 site-packages/torch/_dynamo/convert_frame.py:1233]               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W1127 19:16:03.160000 969 site-packages/torch/_dynamo/convert_frame.py:1233]   File \"/opt/conda/lib/python3.11/site-packages/torch/_dynamo/variables/misc.py\", line 1022, in call_function\n",
      "W1127 19:16:03.160000 969 site-packages/torch/_dynamo/convert_frame.py:1233]     return self.obj.call_method(tx, self.name, args, kwargs)\n",
      "W1127 19:16:03.160000 969 site-packages/torch/_dynamo/convert_frame.py:1233]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W1127 19:16:03.160000 969 site-packages/torch/_dynamo/convert_frame.py:1233]   File \"/opt/conda/lib/python3.11/site-packages/torch/_dynamo/variables/misc.py\", line 759, in call_method\n",
      "W1127 19:16:03.160000 969 site-packages/torch/_dynamo/convert_frame.py:1233]     return self.call_apply(tx, args, kwargs)\n",
      "W1127 19:16:03.160000 969 site-packages/torch/_dynamo/convert_frame.py:1233]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W1127 19:16:03.160000 969 site-packages/torch/_dynamo/convert_frame.py:1233]   File \"/opt/conda/lib/python3.11/site-packages/torch/_dynamo/variables/misc.py\", line 681, in call_apply\n",
      "W1127 19:16:03.160000 969 site-packages/torch/_dynamo/convert_frame.py:1233]     ).call_function(tx, args, kwargs)\n",
      "W1127 19:16:03.160000 969 site-packages/torch/_dynamo/convert_frame.py:1233]       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W1127 19:16:03.160000 969 site-packages/torch/_dynamo/convert_frame.py:1233]   File \"/opt/conda/lib/python3.11/site-packages/torch/_dynamo/variables/higher_order_ops.py\", line 2531, in call_function\n",
      "W1127 19:16:03.160000 969 site-packages/torch/_dynamo/convert_frame.py:1233]     new_fwd_graph_outputs = pytree.tree_map(lambda x: x.node, new_fwd_graph_outputs)\n",
      "W1127 19:16:03.160000 969 site-packages/torch/_dynamo/convert_frame.py:1233]                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W1127 19:16:03.160000 969 site-packages/torch/_dynamo/convert_frame.py:1233]   File \"/opt/conda/lib/python3.11/site-packages/torch/utils/_pytree.py\", line 991, in tree_map\n",
      "W1127 19:16:03.160000 969 site-packages/torch/_dynamo/convert_frame.py:1233]     return treespec.unflatten(map(func, *flat_args))\n",
      "W1127 19:16:03.160000 969 site-packages/torch/_dynamo/convert_frame.py:1233]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W1127 19:16:03.160000 969 site-packages/torch/_dynamo/convert_frame.py:1233]   File \"/opt/conda/lib/python3.11/site-packages/torch/utils/_pytree.py\", line 830, in unflatten\n",
      "W1127 19:16:03.160000 969 site-packages/torch/_dynamo/convert_frame.py:1233]     leaves = list(leaves)\n",
      "W1127 19:16:03.160000 969 site-packages/torch/_dynamo/convert_frame.py:1233]              ^^^^^^^^^^^^\n",
      "W1127 19:16:03.160000 969 site-packages/torch/_dynamo/convert_frame.py:1233]   File \"/opt/conda/lib/python3.11/site-packages/torch/_dynamo/variables/higher_order_ops.py\", line 2531, in <lambda>\n",
      "W1127 19:16:03.160000 969 site-packages/torch/_dynamo/convert_frame.py:1233]     new_fwd_graph_outputs = pytree.tree_map(lambda x: x.node, new_fwd_graph_outputs)\n",
      "W1127 19:16:03.160000 969 site-packages/torch/_dynamo/convert_frame.py:1233]                                                       ^^^^^^\n",
      "W1127 19:16:03.160000 969 site-packages/torch/_dynamo/convert_frame.py:1233] torch._dynamo.exc.InternalTorchDynamoError: AttributeError: 'NoneType' object has no attribute 'node'\n",
      "W1127 19:16:03.160000 969 site-packages/torch/_dynamo/convert_frame.py:1233] \n",
      "W1127 19:16:03.160000 969 site-packages/torch/_dynamo/convert_frame.py:1233] from user code:\n",
      "W1127 19:16:03.160000 969 site-packages/torch/_dynamo/convert_frame.py:1233]    File \"/workspace/AI-Game-Engine/mmfreelm/ops/hgrn/recurrent_fuse.py\", line 184, in fused_recurrent_hgrn\n",
      "W1127 19:16:03.160000 969 site-packages/torch/_dynamo/convert_frame.py:1233]     o, final_state = FusedRecurrentHGRNFunction.apply(x, g, initial_state, output_final_state)\n",
      "W1127 19:16:03.160000 969 site-packages/torch/_dynamo/convert_frame.py:1233] \n",
      "W1127 19:16:03.160000 969 site-packages/torch/_dynamo/convert_frame.py:1233] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
      "W1127 19:16:03.160000 969 site-packages/torch/_dynamo/convert_frame.py:1233] \n",
      "W1127 19:16:03.160000 969 site-packages/torch/_dynamo/convert_frame.py:1233] Traceback (most recent call last):\n",
      "W1127 19:16:03.160000 969 site-packages/torch/_dynamo/convert_frame.py:1233]   File \"/opt/conda/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py\", line 1164, in __call__\n",
      "W1127 19:16:03.160000 969 site-packages/torch/_dynamo/convert_frame.py:1233]     result = self._inner_convert(\n",
      "W1127 19:16:03.160000 969 site-packages/torch/_dynamo/convert_frame.py:1233]              ^^^^^^^^^^^^^^^^^^^^\n",
      "W1127 19:16:03.160000 969 site-packages/torch/_dynamo/convert_frame.py:1233]   File \"/opt/conda/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py\", line 547, in __call__\n",
      "W1127 19:16:03.160000 969 site-packages/torch/_dynamo/convert_frame.py:1233]     return _compile(\n",
      "W1127 19:16:03.160000 969 site-packages/torch/_dynamo/convert_frame.py:1233]            ^^^^^^^^^\n",
      "W1127 19:16:03.160000 969 site-packages/torch/_dynamo/convert_frame.py:1233]   File \"/opt/conda/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py\", line 1036, in _compile\n",
      "W1127 19:16:03.160000 969 site-packages/torch/_dynamo/convert_frame.py:1233]     raise InternalTorchDynamoError(\n",
      "W1127 19:16:03.160000 969 site-packages/torch/_dynamo/convert_frame.py:1233]   File \"/opt/conda/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py\", line 986, in _compile\n",
      "W1127 19:16:03.160000 969 site-packages/torch/_dynamo/convert_frame.py:1233]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
      "W1127 19:16:03.160000 969 site-packages/torch/_dynamo/convert_frame.py:1233]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W1127 19:16:03.160000 969 site-packages/torch/_dynamo/convert_frame.py:1233]   File \"/opt/conda/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py\", line 715, in compile_inner\n",
      "W1127 19:16:03.160000 969 site-packages/torch/_dynamo/convert_frame.py:1233]     return _compile_inner(code, one_graph, hooks, transform)\n",
      "W1127 19:16:03.160000 969 site-packages/torch/_dynamo/convert_frame.py:1233]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W1127 19:16:03.160000 969 site-packages/torch/_dynamo/convert_frame.py:1233]   File \"/opt/conda/lib/python3.11/site-packages/torch/_utils_internal.py\", line 95, in wrapper_function\n",
      "W1127 19:16:03.160000 969 site-packages/torch/_dynamo/convert_frame.py:1233]     return function(*args, **kwargs)\n",
      "W1127 19:16:03.160000 969 site-packages/torch/_dynamo/convert_frame.py:1233]            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W1127 19:16:03.160000 969 site-packages/torch/_dynamo/convert_frame.py:1233]   File \"/opt/conda/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py\", line 750, in _compile_inner\n",
      "W1127 19:16:03.160000 969 site-packages/torch/_dynamo/convert_frame.py:1233]     out_code = transform_code_object(code, transform)\n",
      "W1127 19:16:03.160000 969 site-packages/torch/_dynamo/convert_frame.py:1233]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W1127 19:16:03.160000 969 site-packages/torch/_dynamo/convert_frame.py:1233]   File \"/opt/conda/lib/python3.11/site-packages/torch/_dynamo/bytecode_transformation.py\", line 1361, in transform_code_object\n",
      "W1127 19:16:03.160000 969 site-packages/torch/_dynamo/convert_frame.py:1233]     transformations(instructions, code_options)\n",
      "W1127 19:16:03.160000 969 site-packages/torch/_dynamo/convert_frame.py:1233]   File \"/opt/conda/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py\", line 231, in _fn\n",
      "W1127 19:16:03.160000 969 site-packages/torch/_dynamo/convert_frame.py:1233]     return fn(*args, **kwargs)\n",
      "W1127 19:16:03.160000 969 site-packages/torch/_dynamo/convert_frame.py:1233]            ^^^^^^^^^^^^^^^^^^^\n",
      "W1127 19:16:03.160000 969 site-packages/torch/_dynamo/convert_frame.py:1233]   File \"/opt/conda/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py\", line 662, in transform\n",
      "W1127 19:16:03.160000 969 site-packages/torch/_dynamo/convert_frame.py:1233]     tracer.run()\n",
      "W1127 19:16:03.160000 969 site-packages/torch/_dynamo/convert_frame.py:1233]   File \"/opt/conda/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py\", line 2868, in run\n",
      "W1127 19:16:03.160000 969 site-packages/torch/_dynamo/convert_frame.py:1233]     super().run()\n",
      "W1127 19:16:03.160000 969 site-packages/torch/_dynamo/convert_frame.py:1233]   File \"/opt/conda/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py\", line 1052, in run\n",
      "W1127 19:16:03.160000 969 site-packages/torch/_dynamo/convert_frame.py:1233]     while self.step():\n",
      "W1127 19:16:03.160000 969 site-packages/torch/_dynamo/convert_frame.py:1233]           ^^^^^^^^^^^\n",
      "W1127 19:16:03.160000 969 site-packages/torch/_dynamo/convert_frame.py:1233]   File \"/opt/conda/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py\", line 962, in step\n",
      "W1127 19:16:03.160000 969 site-packages/torch/_dynamo/convert_frame.py:1233]     self.dispatch_table[inst.opcode](self, inst)\n",
      "W1127 19:16:03.160000 969 site-packages/torch/_dynamo/convert_frame.py:1233]   File \"/opt/conda/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py\", line 659, in wrapper\n",
      "W1127 19:16:03.160000 969 site-packages/torch/_dynamo/convert_frame.py:1233]     return inner_fn(self, inst)\n",
      "W1127 19:16:03.160000 969 site-packages/torch/_dynamo/convert_frame.py:1233]            ^^^^^^^^^^^^^^^^^^^^\n",
      "W1127 19:16:03.160000 969 site-packages/torch/_dynamo/convert_frame.py:1233]   File \"/opt/conda/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py\", line 2341, in CALL\n",
      "W1127 19:16:03.160000 969 site-packages/torch/_dynamo/convert_frame.py:1233]     self._call(inst)\n",
      "W1127 19:16:03.160000 969 site-packages/torch/_dynamo/convert_frame.py:1233]   File \"/opt/conda/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py\", line 2335, in _call\n",
      "W1127 19:16:03.160000 969 site-packages/torch/_dynamo/convert_frame.py:1233]     self.call_function(fn, args, kwargs)\n",
      "W1127 19:16:03.160000 969 site-packages/torch/_dynamo/convert_frame.py:1233]   File \"/opt/conda/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py\", line 897, in call_function\n",
      "W1127 19:16:03.160000 969 site-packages/torch/_dynamo/convert_frame.py:1233]     self.push(fn.call_function(self, args, kwargs))  # type: ignore[arg-type]\n",
      "W1127 19:16:03.160000 969 site-packages/torch/_dynamo/convert_frame.py:1233]               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W1127 19:16:03.160000 969 site-packages/torch/_dynamo/convert_frame.py:1233]   File \"/opt/conda/lib/python3.11/site-packages/torch/_dynamo/variables/misc.py\", line 1022, in call_function\n",
      "W1127 19:16:03.160000 969 site-packages/torch/_dynamo/convert_frame.py:1233]     return self.obj.call_method(tx, self.name, args, kwargs)\n",
      "W1127 19:16:03.160000 969 site-packages/torch/_dynamo/convert_frame.py:1233]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W1127 19:16:03.160000 969 site-packages/torch/_dynamo/convert_frame.py:1233]   File \"/opt/conda/lib/python3.11/site-packages/torch/_dynamo/variables/misc.py\", line 759, in call_method\n",
      "W1127 19:16:03.160000 969 site-packages/torch/_dynamo/convert_frame.py:1233]     return self.call_apply(tx, args, kwargs)\n",
      "W1127 19:16:03.160000 969 site-packages/torch/_dynamo/convert_frame.py:1233]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W1127 19:16:03.160000 969 site-packages/torch/_dynamo/convert_frame.py:1233]   File \"/opt/conda/lib/python3.11/site-packages/torch/_dynamo/variables/misc.py\", line 681, in call_apply\n",
      "W1127 19:16:03.160000 969 site-packages/torch/_dynamo/convert_frame.py:1233]     ).call_function(tx, args, kwargs)\n",
      "W1127 19:16:03.160000 969 site-packages/torch/_dynamo/convert_frame.py:1233]       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W1127 19:16:03.160000 969 site-packages/torch/_dynamo/convert_frame.py:1233]   File \"/opt/conda/lib/python3.11/site-packages/torch/_dynamo/variables/higher_order_ops.py\", line 2531, in call_function\n",
      "W1127 19:16:03.160000 969 site-packages/torch/_dynamo/convert_frame.py:1233]     new_fwd_graph_outputs = pytree.tree_map(lambda x: x.node, new_fwd_graph_outputs)\n",
      "W1127 19:16:03.160000 969 site-packages/torch/_dynamo/convert_frame.py:1233]                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W1127 19:16:03.160000 969 site-packages/torch/_dynamo/convert_frame.py:1233]   File \"/opt/conda/lib/python3.11/site-packages/torch/utils/_pytree.py\", line 991, in tree_map\n",
      "W1127 19:16:03.160000 969 site-packages/torch/_dynamo/convert_frame.py:1233]     return treespec.unflatten(map(func, *flat_args))\n",
      "W1127 19:16:03.160000 969 site-packages/torch/_dynamo/convert_frame.py:1233]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W1127 19:16:03.160000 969 site-packages/torch/_dynamo/convert_frame.py:1233]   File \"/opt/conda/lib/python3.11/site-packages/torch/utils/_pytree.py\", line 830, in unflatten\n",
      "W1127 19:16:03.160000 969 site-packages/torch/_dynamo/convert_frame.py:1233]     leaves = list(leaves)\n",
      "W1127 19:16:03.160000 969 site-packages/torch/_dynamo/convert_frame.py:1233]              ^^^^^^^^^^^^\n",
      "W1127 19:16:03.160000 969 site-packages/torch/_dynamo/convert_frame.py:1233]   File \"/opt/conda/lib/python3.11/site-packages/torch/_dynamo/variables/higher_order_ops.py\", line 2531, in <lambda>\n",
      "W1127 19:16:03.160000 969 site-packages/torch/_dynamo/convert_frame.py:1233]     new_fwd_graph_outputs = pytree.tree_map(lambda x: x.node, new_fwd_graph_outputs)\n",
      "W1127 19:16:03.160000 969 site-packages/torch/_dynamo/convert_frame.py:1233]                                                       ^^^^^^\n",
      "W1127 19:16:03.160000 969 site-packages/torch/_dynamo/convert_frame.py:1233] torch._dynamo.exc.InternalTorchDynamoError: AttributeError: 'NoneType' object has no attribute 'node'\n",
      "W1127 19:16:03.160000 969 site-packages/torch/_dynamo/convert_frame.py:1233] \n",
      "W1127 19:16:03.160000 969 site-packages/torch/_dynamo/convert_frame.py:1233] from user code:\n",
      "W1127 19:16:03.160000 969 site-packages/torch/_dynamo/convert_frame.py:1233]    File \"/workspace/AI-Game-Engine/mmfreelm/ops/hgrn/recurrent_fuse.py\", line 184, in fused_recurrent_hgrn\n",
      "W1127 19:16:03.160000 969 site-packages/torch/_dynamo/convert_frame.py:1233]     o, final_state = FusedRecurrentHGRNFunction.apply(x, g, initial_state, output_final_state)\n",
      "W1127 19:16:03.160000 969 site-packages/torch/_dynamo/convert_frame.py:1233] \n",
      "W1127 19:16:03.160000 969 site-packages/torch/_dynamo/convert_frame.py:1233] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
      "W1127 19:16:03.160000 969 site-packages/torch/_dynamo/convert_frame.py:1233] \n",
      "W1127 19:16:49.541000 969 site-packages/torch/_dynamo/convert_frame.py:906] [9/8] torch._dynamo hit config.cache_size_limit (8)\n",
      "W1127 19:16:49.541000 969 site-packages/torch/_dynamo/convert_frame.py:906] [9/8]    function: 'wrapper' (/workspace/AI-Game-Engine/mmfreelm/utils.py:7)\n",
      "W1127 19:16:49.541000 969 site-packages/torch/_dynamo/convert_frame.py:906] [9/8]    last reason: 9/0: tensor 'L['args'][0]' dtype mismatch. expected Half, actual Float\n",
      "W1127 19:16:49.541000 969 site-packages/torch/_dynamo/convert_frame.py:906] [9/8] To log all recompilation reasons, use TORCH_LOGS=\"recompiles\".\n",
      "W1127 19:16:49.541000 969 site-packages/torch/_dynamo/convert_frame.py:906] [9/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1720' max='1720' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1720/1720 3:10:50, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.102800</td>\n",
       "      <td>0.104388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.103100</td>\n",
       "      <td>0.104266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.103500</td>\n",
       "      <td>0.101336</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EMA weights saved to ./phase1_ckpt/checkpoint-1000/pytorch_model_ema.bin\n",
      "EMA weights saved to ./phase1_ckpt/checkpoint-1720/pytorch_model_ema.bin\n",
      "Loading EMA weights into model for final save...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>██▁</td></tr><tr><td>eval/runtime</td><td>█▇▁</td></tr><tr><td>eval/samples_per_second</td><td>▁▁█</td></tr><tr><td>eval/steps_per_second</td><td>▁▁█</td></tr><tr><td>train/epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/grad_norm</td><td>▆█▇▄▄▄▅▅▄▄▇▇▆▆▅▅▆▅▅▆▅▄▃▅▄▄▁▂▂▃▁▁▁▁</td></tr><tr><td>train/learning_rate</td><td>▅██████▇▇▇▇▆▆▆▆▅▅▅▄▄▄▃▃▃▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>train/loss</td><td>▆▆█▄▆▆▇▅▇▄▃█▅▃▅▃▆▄▅▄▂▂▃▅▅▁▃▅▃▅▆▃▅▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.10134</td></tr><tr><td>eval/runtime</td><td>92.3751</td></tr><tr><td>eval/samples_per_second</td><td>105.331</td></tr><tr><td>eval/steps_per_second</td><td>13.175</td></tr><tr><td>total_flos</td><td>0</td></tr><tr><td>train/epoch</td><td>10</td></tr><tr><td>train/global_step</td><td>1720</td></tr><tr><td>train/grad_norm</td><td>0.25448</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.1024</td></tr><tr><td>+4</td><td>...</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MeshDiT-S-Optimized</strong> at: <a href='https://wandb.ai/sakibahmed2018go/mesh-dit-3d-generation/runs/jnz4gkw8' target=\"_blank\">https://wandb.ai/sakibahmed2018go/mesh-dit-3d-generation/runs/jnz4gkw8</a><br> View project at: <a href='https://wandb.ai/sakibahmed2018go/mesh-dit-3d-generation' target=\"_blank\">https://wandb.ai/sakibahmed2018go/mesh-dit-3d-generation</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251127_191538-jnz4gkw8/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TRAINING COMPLETE!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19edc13a-7bb8-43cb-aa04-4544c8f2d032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[main] files: ['model.safetensors', 'training_args.bin', 'tokenizer_config.json', 'special_tokens_map.json', 'tokenizer.json']\n",
      "[slug] 'sakibahmed2022/meshdit-trained-model' exists → using timestamped\n",
      "[meta] ./mesh_dit_final/dataset-metadata.json\n",
      "[main] trying Python API …\n",
      "Starting upload for file model.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 964M/964M [00:06<00:00, 159MB/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: model.safetensors (964MB)\n",
      "Starting upload for file training_args.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5.30k/5.30k [00:00<00:00, 13.2kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: training_args.bin (5KB)\n",
      "Starting upload for file tokenizer_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1.19k/1.19k [00:00<00:00, 3.31kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: tokenizer_config.json (1KB)\n",
      "Starting upload for file special_tokens_map.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 437/437 [00:00<00:00, 998B/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: special_tokens_map.json (437B)\n",
      "Starting upload for file tokenizer.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3.34M/3.34M [00:00<00:00, 7.42MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: tokenizer.json (3MB)\n",
      "[api] 403 after file upload → will use CLI\n",
      "[main] API failed → CLI fallback …\n",
      "[cli] version: kaggle datasets version -p ./mesh_dit_final -m 'Upload final model – 2025-11-27T22:28:40.542092 UTC'\n"
     ]
    },
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'ascii' codec can't decode byte 0xe2 in position 152: ordinal not in range(128)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 127\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBoth API and CLI failed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 127\u001b[0m     final_url \u001b[38;5;241m=\u001b[39m \u001b[43mupload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m=== FINAL URL ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28mprint\u001b[39m(final_url)\n",
      "Cell \u001b[0;32mIn[11], line 117\u001b[0m, in \u001b[0;36mupload\u001b[0;34m()\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;66;03m# 2. CLI\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[main] API failed → CLI fallback …\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 117\u001b[0m ok, final_id \u001b[38;5;241m=\u001b[39m \u001b[43mtry_cli\u001b[49m\u001b[43m(\u001b[49m\u001b[43mLOCAL_DIR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mVERSION_MSG\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ok:\n\u001b[1;32m    119\u001b[0m     url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://www.kaggle.com/datasets/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfinal_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[0;32mIn[11], line 64\u001b[0m, in \u001b[0;36mtry_cli\u001b[0;34m(folder, dataset_id, msg)\u001b[0m\n\u001b[1;32m     62\u001b[0m cmd \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkaggle datasets version -p \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshlex\u001b[38;5;241m.\u001b[39mquote(folder)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m -m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshlex\u001b[38;5;241m.\u001b[39mquote(msg)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[cli] version: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcmd\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 64\u001b[0m p \u001b[38;5;241m=\u001b[39m \u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshell\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcapture_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[cli] version OK\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/subprocess.py:550\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Popen(\u001b[38;5;241m*\u001b[39mpopenargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mas\u001b[39;00m process:\n\u001b[1;32m    549\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 550\u001b[0m         stdout, stderr \u001b[38;5;241m=\u001b[39m \u001b[43mprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcommunicate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    551\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m TimeoutExpired \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    552\u001b[0m         process\u001b[38;5;241m.\u001b[39mkill()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/subprocess.py:1209\u001b[0m, in \u001b[0;36mPopen.communicate\u001b[0;34m(self, input, timeout)\u001b[0m\n\u001b[1;32m   1206\u001b[0m     endtime \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1208\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1209\u001b[0m     stdout, stderr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_communicate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mendtime\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1210\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1211\u001b[0m     \u001b[38;5;66;03m# https://bugs.python.org/issue25942\u001b[39;00m\n\u001b[1;32m   1212\u001b[0m     \u001b[38;5;66;03m# See the detailed comment in .wait().\u001b[39;00m\n\u001b[1;32m   1213\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/subprocess.py:2157\u001b[0m, in \u001b[0;36mPopen._communicate\u001b[0;34m(self, input, endtime, orig_timeout)\u001b[0m\n\u001b[1;32m   2153\u001b[0m         stdout \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_translate_newlines(stdout,\n\u001b[1;32m   2154\u001b[0m                                           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstdout\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[1;32m   2155\u001b[0m                                           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstdout\u001b[38;5;241m.\u001b[39merrors)\n\u001b[1;32m   2156\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stderr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 2157\u001b[0m         stderr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_translate_newlines\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstderr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2158\u001b[0m \u001b[43m                                          \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstderr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2159\u001b[0m \u001b[43m                                          \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstderr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2161\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (stdout, stderr)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/subprocess.py:1086\u001b[0m, in \u001b[0;36mPopen._translate_newlines\u001b[0;34m(self, data, encoding, errors)\u001b[0m\n\u001b[1;32m   1085\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_translate_newlines\u001b[39m(\u001b[38;5;28mself\u001b[39m, data, encoding, errors):\n\u001b[0;32m-> 1086\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1087\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'ascii' codec can't decode byte 0xe2 in position 152: ordinal not in range(128)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import shlex\n",
    "import subprocess\n",
    "from datetime import datetime\n",
    "\n",
    "KAGGLE_USERNAME = \"sakibahmed2022\"\n",
    "KAGGLE_KEY = \"d0ec7fe8091a3906c6995568c11e9e58\"\n",
    "os.environ['KAGGLE_USERNAME'] = KAGGLE_USERNAME\n",
    "os.environ['KAGGLE_KEY'] = KAGGLE_KEY\n",
    "\n",
    "DESIRED_SLUG    = f\"{KAGGLE_USERNAME}/meshdit-trained-model\"\n",
    "LOCAL_DIR       = \"./mesh_dit_final\"\n",
    "TITLE           = \"MeshDiT S Trained Model 10 epoch\"\n",
    "VERSION_MSG     = f\"Upload final model – {datetime.utcnow().isoformat()} UTC\"\n",
    "PUBLIC          = True          \n",
    "\n",
    "\n",
    "def write_metadata(folder: str, dataset_id: str):\n",
    "    meta = {\n",
    "        \"title\": TITLE,\n",
    "        \"id\": dataset_id,\n",
    "        \"licenses\": [{\"name\": \"CC-BY-4.0\"}],\n",
    "        \"description\": f\"{TITLE} – uploaded {datetime.utcnow().isoformat()} UTC\"\n",
    "    }\n",
    "    path = os.path.join(folder, \"dataset-metadata.json\")\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(meta, f, indent=2)\n",
    "    print(f\"[meta] {path}\")\n",
    "\n",
    "\n",
    "def try_api(folder: str, dataset_id: str, msg: str):\n",
    "    from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "    api = KaggleApi()\n",
    "    api.authenticate()\n",
    "\n",
    "    try:\n",
    "        api.dataset_create_version(\n",
    "            folder=folder,\n",
    "            version_notes=msg,\n",
    "            convert_to_csv=False,\n",
    "            delete_old_versions=False,\n",
    "        )\n",
    "        print(\"[api] version created\")\n",
    "        return True, dataset_id\n",
    "    except Exception as e:\n",
    "        if \"403\" in str(e):\n",
    "            print(\"[api] 403 after file upload → will use CLI\")\n",
    "            return False, dataset_id\n",
    "        # Dataset missing → create new\n",
    "        if \"does not exist\" in str(e).lower() or \"404\" in str(e):\n",
    "            print(\"[api] dataset missing → creating new\")\n",
    "            api.dataset_create_new(folder=folder, public=PUBLIC, convert_to_csv=False)\n",
    "            print(\"[api] new dataset created\")\n",
    "            return True, dataset_id\n",
    "        print(f\"[api] unexpected error: {e}\")\n",
    "        return False, None\n",
    "\n",
    "\n",
    "def try_cli(folder: str, dataset_id: str, msg: str):\n",
    "    # 1. version\n",
    "    cmd = f\"kaggle datasets version -p {shlex.quote(folder)} -m {shlex.quote(msg)}\"\n",
    "    print(f\"[cli] version: {cmd}\")\n",
    "    p = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n",
    "    if p.returncode == 0:\n",
    "        print(\"[cli] version OK\")\n",
    "        return True, dataset_id\n",
    "\n",
    "    # 2. create (only if version failed because dataset missing)\n",
    "    cmd = f\"kaggle datasets create -p {shlex.quote(folder)}\"\n",
    "    print(f\"[cli] create: {cmd}\")\n",
    "    p = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n",
    "    if p.returncode == 0:\n",
    "        print(\"[cli] create OK\")\n",
    "        return True, dataset_id\n",
    "\n",
    "    print(\"[cli] both commands failed\")\n",
    "    print(\"STDOUT:\", p.stdout)\n",
    "    print(\"STDERR:\", p.stderr)\n",
    "    return False, None\n",
    "\n",
    "\n",
    "def upload():\n",
    "    if not os.path.isdir(LOCAL_DIR):\n",
    "        raise FileNotFoundError(LOCAL_DIR)\n",
    "\n",
    "    files = [f for f in os.listdir(LOCAL_DIR) if os.path.isfile(os.path.join(LOCAL_DIR, f))]\n",
    "    print(\"[main] files:\", files)\n",
    "\n",
    "    # Resolve slug\n",
    "    dataset_id = DESIRED_SLUG\n",
    "    ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "    alt = f\"{KAGGLE_USERNAME}/meshdit-trained-model-{ts}\"\n",
    "\n",
    "    # Try a cheap API call to see if the slug exists\n",
    "    try:\n",
    "        from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "        KaggleApi().authenticate()\n",
    "        KaggleApi().dataset_list_files(dataset_id)   # raises if missing\n",
    "        print(f\"[slug] '{dataset_id}' exists → using timestamped\")\n",
    "        dataset_id = alt\n",
    "    except Exception:\n",
    "        pass   # keep desired slug\n",
    "\n",
    "    write_metadata(LOCAL_DIR, dataset_id)\n",
    "\n",
    "    # 1. API\n",
    "    print(\"[main] trying Python API …\")\n",
    "    ok, final_id = try_api(LOCAL_DIR, dataset_id, VERSION_MSG)\n",
    "    if ok:\n",
    "        url = f\"https://www.kaggle.com/datasets/{final_id}\"\n",
    "        print(f\"[SUCCESS] API → {url}\")\n",
    "        return url\n",
    "\n",
    "    # 2. CLI\n",
    "    print(\"[main] API failed → CLI fallback …\")\n",
    "    ok, final_id = try_cli(LOCAL_DIR, dataset_id, VERSION_MSG)\n",
    "    if ok:\n",
    "        url = f\"https://www.kaggle.com/datasets/{final_id}\"\n",
    "        print(f\"[SUCCESS] CLI → {url}\")\n",
    "        return url\n",
    "\n",
    "    raise RuntimeError(\"Both API and CLI failed\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    final_url = upload()\n",
    "    print(\"\\n=== FINAL URL ===\")\n",
    "    print(final_url)\n",
    "    print(\"\\nDownload with:\")\n",
    "    print(f\"kaggle datasets download -d {final_url.split('/')[-2]}/{final_url.split('/')[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45ba9eb-dc4c-42b6-9b38-abf460bd12c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
