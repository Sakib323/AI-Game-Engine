{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: einops in /opt/conda/lib/python3.11/site-packages (0.8.1)\n",
      "Requirement already satisfied: timm in /opt/conda/lib/python3.11/site-packages (1.0.22)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.11/site-packages (from timm) (2.6.0+cu124)\n",
      "Requirement already satisfied: torchvision in /opt/conda/lib/python3.11/site-packages (from timm) (0.21.0+cu124)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.11/site-packages (from timm) (6.0.2)\n",
      "Requirement already satisfied: huggingface_hub in /opt/conda/lib/python3.11/site-packages (from timm) (0.36.0)\n",
      "Requirement already satisfied: safetensors in /opt/conda/lib/python3.11/site-packages (from timm) (0.7.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from huggingface_hub->timm) (3.17.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.11/site-packages (from huggingface_hub->timm) (2024.12.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.11/site-packages (from huggingface_hub->timm) (24.2)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from huggingface_hub->timm) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.11/site-packages (from huggingface_hub->timm) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.11/site-packages (from huggingface_hub->timm) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/conda/lib/python3.11/site-packages (from huggingface_hub->timm) (1.2.0)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch->timm) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch->timm) (3.1.5)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch->timm) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch->timm) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch->timm) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /opt/conda/lib/python3.11/site-packages (from torch->timm) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /opt/conda/lib/python3.11/site-packages (from torch->timm) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /opt/conda/lib/python3.11/site-packages (from torch->timm) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /opt/conda/lib/python3.11/site-packages (from torch->timm) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /opt/conda/lib/python3.11/site-packages (from torch->timm) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /opt/conda/lib/python3.11/site-packages (from torch->timm) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /opt/conda/lib/python3.11/site-packages (from torch->timm) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /opt/conda/lib/python3.11/site-packages (from torch->timm) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch->timm) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch->timm) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /opt/conda/lib/python3.11/site-packages (from torch->timm) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/conda/lib/python3.11/site-packages (from torch->timm) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from sympy==1.13.1->torch->timm) (1.3.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.11/site-packages (from torchvision->timm) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.11/site-packages (from torchvision->timm) (9.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch->timm) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface_hub->timm) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface_hub->timm) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface_hub->timm) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface_hub->timm) (2024.12.14)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mfatal: destination path 'AI-Game-Engine' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!pip install einops timm\n",
    "import sys, os\n",
    "!git clone https://github.com/Sakib323/AI-Game-Engine.git\n",
    "sys.path.append('/workspace/AI-Game-Engine') \n",
    "from mmfreelm.models.hgrn_bit.mesh_dit import MeshDiT_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Failed to detect the name of this notebook. You can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msakibahmed2018go\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¥ Downloading Sakib323/panda-70m-latents to local cache...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f00006e7afd646bebe96481c1a1d7f72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 28 files:   0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Data downloaded to: /root/.cache/huggingface/hub/datasets--Sakib323--panda-70m-latents/snapshots/50f4d6d292a176fee09d631edded256e01f6c0e4\n",
      "Initializing Dataset...\n",
      "Scanning 28 files to build index map...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:08<00:00,  3.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples found: 11597\n",
      "Train: 10437 | Eval: 1160\n",
      "Loading Tokenizer...\n",
      "Initializing VideoDiT Model...\n",
      "Initializing RotaryEmbedding with theta=10000.0 and ternary=True\n",
      "\n",
      "[RotaryEmbedding] Initialized with: dim=64, max_pos=2048, base=10000.0, ternary=True\n",
      "\n",
      "Initializing RotaryEmbedding with theta=10000.0 and ternary=True\n",
      "\n",
      "[RotaryEmbedding] Initialized with: dim=64, max_pos=2048, base=10000.0, ternary=True\n",
      "\n",
      "Initializing RotaryEmbedding with theta=10000.0 and ternary=True\n",
      "\n",
      "[RotaryEmbedding] Initialized with: dim=64, max_pos=2048, base=10000.0, ternary=True\n",
      "\n",
      "Initializing RotaryEmbedding with theta=10000.0 and ternary=True\n",
      "\n",
      "[RotaryEmbedding] Initialized with: dim=64, max_pos=2048, base=10000.0, ternary=True\n",
      "\n",
      "Initializing RotaryEmbedding with theta=10000.0 and ternary=True\n",
      "\n",
      "[RotaryEmbedding] Initialized with: dim=64, max_pos=2048, base=10000.0, ternary=True\n",
      "\n",
      "Initializing RotaryEmbedding with theta=10000.0 and ternary=True\n",
      "\n",
      "[RotaryEmbedding] Initialized with: dim=64, max_pos=2048, base=10000.0, ternary=True\n",
      "\n",
      "Initializing RotaryEmbedding with theta=10000.0 and ternary=True\n",
      "\n",
      "[RotaryEmbedding] Initialized with: dim=64, max_pos=2048, base=10000.0, ternary=True\n",
      "\n",
      "Initializing RotaryEmbedding with theta=10000.0 and ternary=True\n",
      "\n",
      "[RotaryEmbedding] Initialized with: dim=64, max_pos=2048, base=10000.0, ternary=True\n",
      "\n",
      "Initializing RotaryEmbedding with theta=10000.0 and ternary=True\n",
      "\n",
      "[RotaryEmbedding] Initialized with: dim=64, max_pos=2048, base=10000.0, ternary=True\n",
      "\n",
      "Initializing RotaryEmbedding with theta=10000.0 and ternary=True\n",
      "\n",
      "[RotaryEmbedding] Initialized with: dim=64, max_pos=2048, base=10000.0, ternary=True\n",
      "\n",
      "Initializing RotaryEmbedding with theta=10000.0 and ternary=True\n",
      "\n",
      "[RotaryEmbedding] Initialized with: dim=64, max_pos=2048, base=10000.0, ternary=True\n",
      "\n",
      "Initializing RotaryEmbedding with theta=10000.0 and ternary=True\n",
      "\n",
      "[RotaryEmbedding] Initialized with: dim=64, max_pos=2048, base=10000.0, ternary=True\n",
      "\n",
      "Initializing RotaryEmbedding with theta=10000.0 and ternary=True\n",
      "\n",
      "[RotaryEmbedding] Initialized with: dim=64, max_pos=2048, base=10000.0, ternary=True\n",
      "\n",
      "Initializing RotaryEmbedding with theta=10000.0 and ternary=True\n",
      "\n",
      "[RotaryEmbedding] Initialized with: dim=64, max_pos=2048, base=10000.0, ternary=True\n",
      "\n",
      "Initializing RotaryEmbedding with theta=10000.0 and ternary=True\n",
      "\n",
      "[RotaryEmbedding] Initialized with: dim=64, max_pos=2048, base=10000.0, ternary=True\n",
      "\n",
      "Initializing RotaryEmbedding with theta=10000.0 and ternary=True\n",
      "\n",
      "[RotaryEmbedding] Initialized with: dim=64, max_pos=2048, base=10000.0, ternary=True\n",
      "\n",
      "Initializing RotaryEmbedding with theta=10000.0 and ternary=True\n",
      "\n",
      "[RotaryEmbedding] Initialized with: dim=64, max_pos=2048, base=10000.0, ternary=True\n",
      "\n",
      "Initializing RotaryEmbedding with theta=10000.0 and ternary=True\n",
      "\n",
      "[RotaryEmbedding] Initialized with: dim=64, max_pos=2048, base=10000.0, ternary=True\n",
      "\n",
      "Starting Stable Training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/wandb/run-20251125_104054-08c132xs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/sakibahmed2018go/video-dit-3d-generation/runs/08c132xs' target=\"_blank\">VideoDiT-S-HF-Load</a></strong> to <a href='https://wandb.ai/sakibahmed2018go/video-dit-3d-generation' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/sakibahmed2018go/video-dit-3d-generation' target=\"_blank\">https://wandb.ai/sakibahmed2018go/video-dit-3d-generation</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/sakibahmed2018go/video-dit-3d-generation/runs/08c132xs' target=\"_blank\">https://wandb.ai/sakibahmed2018go/video-dit-3d-generation/runs/08c132xs</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='27' max='1740' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  27/1740 02:39 < 3:02:23, 0.16 it/s, Epoch 0.03/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import glob\n",
    "import bisect\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    Trainer, \n",
    "    TrainingArguments, \n",
    "    default_data_collator\n",
    ")\n",
    "from huggingface_hub import snapshot_download\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "import torch.nn.functional as F  # <--- THIS WAS MISSING\n",
    "from mmfreelm.models.hgrn_bit.video_gen import VideoDiT_models, flow_matching_loss\n",
    "\n",
    "# --- Setup ---\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "MODEL_SAVE_DIR = \"/kaggle/working/checkpoints\"\n",
    "TOKENIZER_NAME = \"Sakib323/MMfreeLM-370M\"\n",
    "HF_DATASET_ID = \"Sakib323/panda-70m-latents\"\n",
    "\n",
    "# --- CONFIG ---\n",
    "BATCH_SIZE = 3 \n",
    "GRADIENT_ACCUMULATION_STEPS = 4 \n",
    "LEARNING_RATE = 5e-5             \n",
    "NUM_EPOCHS = 2\n",
    "NUM_WORKERS = 2\n",
    "INPUT_SIZE = (16, 72, 128) \n",
    "PATCH_SIZE = (1, 2, 2)\n",
    "\n",
    "# WandB Login\n",
    "WANDB_TOKEN = \"89b06c10468af620747b4bd340f72fa5d56f6849\"\n",
    "wandb.login(key=WANDB_TOKEN)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "os.makedirs(MODEL_SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# --- 1. Download Data from Hub ---\n",
    "print(f\"ðŸ“¥ Downloading {HF_DATASET_ID} to local cache...\")\n",
    "# This downloads the .pt files to a local folder managed by HF\n",
    "local_data_dir = snapshot_download(\n",
    "    repo_id=HF_DATASET_ID, \n",
    "    repo_type=\"dataset\",\n",
    "    token=WANDB_TOKEN, # Using the same token variable if it's your HF token too, otherwise use HF token\n",
    "    allow_patterns=[\"*.pt\"] # Only download the data files\n",
    ")\n",
    "print(f\"âœ… Data downloaded to: {local_data_dir}\")\n",
    "\n",
    "# --- 2. Robust Dataset Class (Fixed for variable frame counts) ---\n",
    "class VideoLatentDataset(Dataset):\n",
    "    def __init__(self, data_dir):\n",
    "        self.files = sorted(glob.glob(os.path.join(data_dir, \"*.pt\")))\n",
    "        self.scale_factor = 0.18215 \n",
    "        self.target_frames = 16  # <--- Target frame count\n",
    "        \n",
    "        # Build index map for chunks\n",
    "        self.file_map = []     \n",
    "        self.file_starts = []  \n",
    "        self.total_samples = 0\n",
    "        \n",
    "        print(f\"Scanning {len(self.files)} files to build index map...\")\n",
    "        for f_path in tqdm(self.files):\n",
    "            try:\n",
    "                data = torch.load(f_path, map_location=\"cpu\")\n",
    "                if isinstance(data, list):\n",
    "                    count = len(data)\n",
    "                    is_list = True\n",
    "                else:\n",
    "                    count = 1\n",
    "                    is_list = False\n",
    "                \n",
    "                self.file_starts.append(self.total_samples)\n",
    "                self.file_map.append({\"path\": f_path, \"is_list\": is_list})\n",
    "                self.total_samples += count\n",
    "                del data\n",
    "            except Exception as e:\n",
    "                print(f\"Skipping broken file {f_path}: {e}\")\n",
    "                \n",
    "        print(f\"Total samples found: {self.total_samples}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.total_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Find file index\n",
    "        file_idx = bisect.bisect_right(self.file_starts, idx) - 1\n",
    "        path_info = self.file_map[file_idx]\n",
    "        start_idx = self.file_starts[file_idx]\n",
    "        \n",
    "        # Load data\n",
    "        item_data = torch.load(path_info[\"path\"], map_location=\"cpu\")\n",
    "        \n",
    "        # Extract item\n",
    "        if path_info[\"is_list\"]:\n",
    "            local_idx = idx - start_idx\n",
    "            item = item_data[local_idx]\n",
    "        else:\n",
    "            item = item_data\n",
    "            \n",
    "        # Prepare tensors\n",
    "        latents = item[\"video_latents\"].float()\n",
    "        \n",
    "        # --- FIX: HANDLE VARIABLE FRAME COUNTS ---\n",
    "        # Current shape: (T, C, H, W) e.g., (15, 4, 72, 128)\n",
    "        current_frames = latents.shape[0]\n",
    "        \n",
    "        if current_frames > self.target_frames:\n",
    "            # Too long: Cut it\n",
    "            latents = latents[:self.target_frames]\n",
    "        elif current_frames < self.target_frames:\n",
    "            # Too short: Repeat the last frame to fill\n",
    "            diff = self.target_frames - current_frames\n",
    "            last_frame = latents[-1].unsqueeze(0) # (1, C, H, W)\n",
    "            padding = last_frame.repeat(diff, 1, 1, 1)\n",
    "            latents = torch.cat([latents, padding], dim=0)\n",
    "            \n",
    "        # Now shape is guaranteed to be (16, 4, 72, 128)\n",
    "        \n",
    "        # Permute to (C, T, H, W) for model\n",
    "        latents = latents.permute(1, 0, 2, 3) \n",
    "        latents = latents * self.scale_factor\n",
    "        \n",
    "        return {\n",
    "            \"latents\": latents,\n",
    "            \"input_ids\": item[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": item[\"attention_mask\"].squeeze(0)\n",
    "        }\n",
    "\n",
    "\n",
    "class FlowMatchingTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        x_1 = inputs[\"latents\"]\n",
    "        cond_y = {\n",
    "            \"input_ids\": inputs[\"input_ids\"],\n",
    "            \"attention_mask\": inputs[\"attention_mask\"]\n",
    "        }\n",
    "        \n",
    "        b = x_1.shape[0]\n",
    "        device = x_1.device\n",
    "        x_0 = torch.randn_like(x_1)\n",
    "        t_step = torch.rand(b, device=device)\n",
    "        t_expand = t_step.view(b, 1, 1, 1, 1)\n",
    "        x_t = t_expand * x_1 + (1 - t_expand) * x_0\n",
    "        v_target = x_1 - x_0\n",
    "        v_pred = model(x_t, t_step, cond_y)\n",
    "        loss = F.mse_loss(v_pred, v_target)\n",
    "\n",
    "        if torch.isnan(loss) or torch.isinf(loss):\n",
    "            loss = torch.tensor(0.0, device=device, requires_grad=True)\n",
    "\n",
    "        return (loss, v_pred) if return_outputs else loss\n",
    "\n",
    "# --- 4. Execution ---\n",
    "print(\"Initializing Dataset...\")\n",
    "# Point the dataset to the downloaded snapshot folder\n",
    "full_dataset = VideoLatentDataset(local_data_dir)\n",
    "\n",
    "if len(full_dataset) == 0:\n",
    "    raise ValueError(\"Dataset is empty! Check your Hugging Face repo contains .pt files.\")\n",
    "\n",
    "train_size = int(0.9 * len(full_dataset))\n",
    "eval_size = len(full_dataset) - train_size\n",
    "generator = torch.Generator().manual_seed(42)\n",
    "train_dataset, eval_dataset = torch.utils.data.random_split(full_dataset, [train_size, eval_size], generator=generator)\n",
    "\n",
    "print(f\"Train: {len(train_dataset)} | Eval: {len(eval_dataset)}\")\n",
    "\n",
    "print(\"Loading Tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_NAME)\n",
    "\n",
    "os.environ[\"WANDB_PROJECT\"] = \"video-dit-3d-generation\"\n",
    "\n",
    "print(\"Initializing VideoDiT Model...\")\n",
    "model = VideoDiT_models['VideoDiT-S'](\n",
    "    input_size=INPUT_SIZE,\n",
    "    patch_size=PATCH_SIZE,\n",
    "    in_channels=4, \n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    use_rope=True,\n",
    "    use_ternary_rope=True,\n",
    "    first_frame_condition=False,\n",
    "    full_precision=True,\n",
    "    optimized_bitlinear=False,\n",
    "    use_temporal=False,\n",
    "    use_grid=False,\n",
    "    use_resampling=False,\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=MODEL_SAVE_DIR,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    warmup_ratio=0.1,\n",
    "    max_grad_norm=1.0,           \n",
    "    lr_scheduler_type=\"cosine\",  \n",
    "    weight_decay=0.01,\n",
    "    fp16=True,                   \n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=500,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",    \n",
    "    dataloader_num_workers=NUM_WORKERS,\n",
    "    dataloader_pin_memory=True,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=10,\n",
    "    report_to=\"wandb\",\n",
    "    run_name=\"VideoDiT-S-HF-Load\",\n",
    "    remove_unused_columns=False,\n",
    "    label_names=[\"latents\"],\n",
    ")\n",
    "\n",
    "trainer = FlowMatchingTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=default_data_collator,\n",
    ")\n",
    "\n",
    "print(\"Starting Stable Training...\")\n",
    "trainer.train()\n",
    "trainer.save_model(os.path.join(MODEL_SAVE_DIR, \"final_model\"))\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
