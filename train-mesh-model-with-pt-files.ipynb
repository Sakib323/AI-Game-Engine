{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":13754661,"sourceType":"datasetVersion","datasetId":8752486},{"sourceId":13901316,"sourceType":"datasetVersion","datasetId":8712427}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124\n!pip install transformers\n!pip install datasets\n!pip install wandb\n!pip install -U datasets\n!pip install objaverse\n!pip install diffusers\n!pip install trimesh\n!pip install jaxtyping\n!pip install pytorch-lightning\n!pip install ijson\n!pip install triton==3.2.0\n!pip install wandb\n!pip uninstall -y pillow\n!pip install pillow==9.5.0 --no-cache-dir","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install torch-cluster -f https://data.pyg.org/whl/torch-$(python -c \"import torch; print(torch.__version__)\").html\n!apt-get update && apt-get install -y libaio-dev","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import sys\n!rm -rf AI-Game-Engine\n!git clone https://github.com/Sakib323/AI-Game-Engine.git\nsys.path.append('/kaggle/working/AI-Game-Engine')\nfrom mmfreelm.models.hgrn_bit.mesh_dit import MeshDiT_models","metadata":{"execution":{"iopub.status.busy":"2025-11-29T17:57:43.632238Z","iopub.execute_input":"2025-11-29T17:57:43.632899Z","iopub.status.idle":"2025-11-29T17:58:13.733645Z","shell.execute_reply.started":"2025-11-29T17:57:43.632858Z","shell.execute_reply":"2025-11-29T17:58:13.732779Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Cloning into 'AI-Game-Engine'...\nremote: Enumerating objects: 1249, done.\u001b[K\nremote: Counting objects: 100% (254/254), done.\u001b[K\nremote: Compressing objects: 100% (157/157), done.\u001b[K\nremote: Total 1249 (delta 199), reused 149 (delta 97), pack-reused 995 (from 1)\u001b[K\nReceiving objects: 100% (1249/1249), 463.43 MiB | 44.54 MiB/s, done.\nResolving deltas: 100% (846/846), done.\nUpdating files: 100% (242/242), done.\n","output_type":"stream"},{"name":"stderr","text":"2025-11-29 17:58:06.268004: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1764439086.294014    1996 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1764439086.301370    1996 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import requests\nimport gzip\nimport json\nimport objaverse\nimport gc\nimport os\nimport ijson\nimport objaverse\nimport pathlib\nimport shutil\nimport json\nimport torch\nimport torch.nn.functional as F\nimport traceback\nimport numpy as np\nimport trimesh\nfrom tqdm import tqdm\nimport logging\nimport random\nfrom torch.utils.data import Dataset\nfrom transformers import AutoTokenizer, Trainer, TrainingArguments\nfrom safetensors.torch import load_file as safetensors_load\nimport wandb\nimport shutil\nfrom diffusers import DDIMScheduler\nfrom mmfreelm.models.hgrn_bit.mesh_dit import MeshDiT_models\nimport signal\nimport time\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\nimport psutil\nimport warnings\nimport zipfile\nimport shutil\nfrom kaggle.api.kaggle_api_extended import KaggleApi\nimport os\nimport json\nfrom kaggle.api.kaggle_api_extended import KaggleApi\nimport zipfile\nimport kagglehub\nfrom contextlib import redirect_stdout\nfrom io import StringIO\nimport signal\nimport time\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndtype = torch.float32\nprint(f\"Using device: {device}, dtype: {dtype}\")\n\nprint(\"Loading tokenizer...\")\ntokenizer = AutoTokenizer.from_pretrained(\"Sakib323/MMfreeLM-370M\")\ntokenizer.pad_token = tokenizer.eos_token\nprint(\"Tokenizer loaded.\")","metadata":{"execution":{"iopub.status.busy":"2025-11-29T17:58:39.639004Z","iopub.execute_input":"2025-11-29T17:58:39.639856Z","iopub.status.idle":"2025-11-29T17:58:40.118922Z","shell.execute_reply.started":"2025-11-29T17:58:39.639824Z","shell.execute_reply":"2025-11-29T17:58:40.118086Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Using device: cuda, dtype: torch.float32\nLoading tokenizer...\nTokenizer loaded.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"def train_model():\n    import os, random, shutil\n    import torch\n    import torch.nn.functional as F\n    from torch.utils.data import Dataset, Subset, RandomSampler, SequentialSampler, DataLoader\n    from transformers import Trainer, TrainingArguments\n    from diffusers import DDIMScheduler\n    from tqdm.notebook import tqdm\n    import wandb\n\n\n    WANDB_TOKEN = \"89b06c10468af620747b4bd340f72fa5d56f6849\"\n    try:\n        wandb.login(key=WANDB_TOKEN)\n        os.environ[\"WANDB_PROJECT\"] = \"mesh-dit-3d-generation\"\n        use_wandb = True\n        print(\"Successfully logged in to W&B.\")\n    except Exception as e:\n        print(f\"Could not log in to W&B: {e}. Disabling W&B.\")\n        use_wandb = False\n\n    print(\"\\nCopying 4× dataset from Kaggle input → /tmp ...\")\n    kaggle_input_dir = \"/kaggle/input/meshdit-all-data-5-pt\"\n    cache_dir = \"/tmp/meshdit_cache\"\n    os.makedirs(cache_dir, exist_ok=True)\n    file_names = [\"all_data.pt\",\"all_data_1.pt\",\"all_data_2.pt\",\"all_data_3.pt\",\"all_data_4.pt\"]\n    local_paths = []\n    for name in file_names:\n        src = f\"{kaggle_input_dir}/{name}\"\n        dst = f\"{cache_dir}/{name}\"\n        if os.path.exists(dst):\n            size_gb = os.path.getsize(dst) / (1024**3)\n            print(f\"Already cached: {name} ({size_gb:.2f} GB)\")\n        else:\n            size_gb = os.path.getsize(src) / (1024**3)\n            print(f\"Copying {name} ({size_gb:.2f} GB) → /tmp ...\")\n            shutil.copy2(src, dst)\n        local_paths.append(dst)\n    print(f\"All 4 files ready in {cache_dir}\")\n\n\n    class CustomDataCollator:\n        def __call__(self, features):\n            batch = {}\n            batch['x'] = torch.stack([f['x'] for f in features])\n            y_features = [f['y'] for f in features]\n            batch['y'] = {key: torch.stack([d[key] for d in y_features]) for key in y_features[0]}\n            return batch\n\n    class MeshDiTTrainer(Trainer):\n        def __init__(self, *args, noise_scheduler, **kwargs):\n            super().__init__(*args, **kwargs)\n            self.noise_scheduler = noise_scheduler\n\n        def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n            x_start = inputs.get(\"x\")\n            model_kwargs = inputs.get(\"y\")\n            noise = torch.randn_like(x_start)\n            timesteps = torch.randint(0, self.noise_scheduler.config.num_train_timesteps, (x_start.shape[0],), device=x_start.device).long()\n            noisy_latents = self.noise_scheduler.add_noise(x_start, noise, timesteps)\n            noise_pred = model(noisy_latents, timesteps, model_kwargs)\n            loss = F.huber_loss(noise_pred, noise, delta=1.0)\n            return (loss, {\"loss\": loss}) if return_outputs else loss\n\n        def prediction_step(self, model, inputs, prediction_loss_only, ignore_keys=None):\n            inputs = self._prepare_inputs(inputs)\n            with torch.no_grad():\n                loss = self.compute_loss(model, inputs)\n            return (loss.detach(), None, None)\n\n\n    class ConcatenatedMeshDataset(Dataset):\n        def __init__(self, pt_paths):\n            self.pt_paths = pt_paths\n            self.cumsum = [0]\n            self.lengths = []\n            self.file_handles = []\n            for path in pt_paths:\n                print(f\"Opening {os.path.basename(path)} with mmap (zero copy)...\")\n                handle = torch.load(path, map_location='cpu', mmap=True)\n                data_list = handle['data']\n                length = len(data_list)\n                self.lengths.append(length)\n                self.cumsum.append(self.cumsum[-1] + length)\n                self.file_handles.append(data_list)\n            print(f\"Total samples: {sum(self.lengths):,} across {len(pt_paths)} files\")\n\n        def __len__(self):\n            return self.cumsum[-1]\n\n        def __getitem__(self, idx):\n            for i, cum in enumerate(self.cumsum):\n                if idx < cum:\n                    file_idx = i - 1\n                    local_idx = idx - self.cumsum[file_idx]\n                    break\n            data_list = self.file_handles[file_idx]\n            sample = data_list[local_idx]\n            return {'x': sample['x'], 'y': sample['y']}\n\n    full_dataset = ConcatenatedMeshDataset(local_paths)\n    total_len = len(full_dataset)\n    eval_size = max(1, int(total_len * 0.10))\n    train_size = total_len - eval_size\n    train_subset = Subset(full_dataset, range(eval_size, total_len))\n    eval_subset = Subset(full_dataset, range(eval_size))\n    generator = torch.Generator().manual_seed(42)\n    train_sampler = RandomSampler(train_subset, generator=generator)\n    eval_sampler = SequentialSampler(eval_subset)\n    print(f\"Train: {train_size:,} | Eval: {eval_size:,}\")\n\n    data_collator = CustomDataCollator()\n    noise_scheduler = DDIMScheduler(\n        num_train_timesteps=1000,\n        beta_schedule=\"scaled_linear\",\n        prediction_type=\"epsilon\",\n        clip_sample=False,\n    )\n\n\n    print(\"\\n\" + \"=\"*6060)\n    print(\"PHASE 1: Training from scratch (FAST!)\")\n    print(\"=\"*60)\n\n    model_p1 = MeshDiT_models['MeshDiT-S'](\n        input_tokens=2048,\n        vocab_size=tokenizer.vocab_size,\n        use_rope=True,\n        use_ternary_rope=True,\n        image_condition=False,\n        full_precision=True,\n        optimized_bitlinear=False,\n    ).to(device, dtype=dtype)\n    \n    #state_dict_p1 = safetensors_load(\"/kaggle/input/meshdit-trained-model/model.safetensors\", device=\"cpu\")\n    #model_p1.load_state_dict(state_dict_p1)\n    #print(\"✅ Phase 1 weights loaded for fine-tuning\")\n    \n\n    print(model_p1)\n\n    training_args_p1 = TrainingArguments(\n        output_dir=\"./phase3_ckpt\",\n        num_train_epochs=3,\n        per_device_train_batch_size=8, \n        gradient_accumulation_steps=8,\n        learning_rate=1e-4,\n        lr_scheduler_type=\"cosine\",  \n        weight_decay=0.01,\n        warmup_ratio=0.1,        \n        logging_steps=50,\n        eval_strategy=\"steps\",\n        eval_steps=500,\n        save_strategy=\"steps\",\n        save_steps=500,\n        save_total_limit=3,\n        load_best_model_at_end=True,\n        metric_for_best_model=\"eval_loss\",\n        greater_is_better=False,\n        fp16=True,\n        max_grad_norm=5.0,          \n        dataloader_num_workers=4,\n        remove_unused_columns=False,\n        report_to=\"wandb\" if use_wandb else \"tensorboard\",\n        run_name=\"MeshDiT-S-Phase1-Final\",\n        dataloader_pin_memory=True,\n        dataloader_prefetch_factor=4,\n    )\n\n    trainer_p1 = MeshDiTTrainer(\n        model=model_p1,\n        args=training_args_p1,\n        train_dataset=train_subset,\n        eval_dataset=eval_subset,\n        data_collator=data_collator,\n        noise_scheduler=noise_scheduler,\n    )\n\n    trainer_p1.train_dataloader = DataLoader(\n        train_subset,\n        batch_size=training_args_p1.per_device_train_batch_size,\n        sampler=train_sampler,\n        collate_fn=data_collator,\n        num_workers=2,\n        pin_memory=True,\n        prefetch_factor=8,\n        persistent_workers=True,\n    )\n    trainer_p1.eval_dataloader = DataLoader(\n        eval_subset,\n        batch_size=training_args_p1.per_device_train_batch_size,\n        sampler=eval_sampler,\n        collate_fn=data_collator,\n        num_workers=2,\n        pin_memory=True,\n    )\n\n    trainer_p1.train()\n    trainer_p1.save_model(\"./mesh_dit_final\")\n    tokenizer.save_pretrained(\"./mesh_dit_final\")\n\n    print(\"\\nTRAINING 100% COMPLETE!\")","metadata":{"execution":{"iopub.status.busy":"2025-11-29T17:58:43.722211Z","iopub.execute_input":"2025-11-29T17:58:43.722571Z","iopub.status.idle":"2025-11-29T17:58:43.745305Z","shell.execute_reply.started":"2025-11-29T17:58:43.722547Z","shell.execute_reply":"2025-11-29T17:58:43.744373Z"},"trusted":true},"outputs":[],"execution_count":4},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    train_model()","metadata":{"execution":{"iopub.status.busy":"2025-11-29T17:58:49.460567Z","iopub.execute_input":"2025-11-29T17:58:49.461288Z"},"trusted":true},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msakibahmed2018go\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"name":"stdout","text":"Successfully logged in to W&B.\n\nCopying 4× dataset from Kaggle input → /tmp ...\nAlready cached: all_data.pt (9.54 GB)\nAlready cached: all_data_1.pt (8.79 GB)\nAlready cached: all_data_2.pt (9.87 GB)\nAlready cached: all_data_3.pt (9.91 GB)\nAlready cached: all_data_4.pt (9.66 GB)\nAll 4 files ready in /tmp/meshdit_cache\nOpening all_data.pt with mmap (zero copy)...\nOpening all_data_1.pt with mmap (zero copy)...\nOpening all_data_2.pt with mmap (zero copy)...\nOpening all_data_3.pt with mmap (zero copy)...\nOpening all_data_4.pt with mmap (zero copy)...\nTotal samples: 97,308 across 5 files\nTrain: 87,578 | Eval: 9,730\n\n============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================\nPHASE 1: Training from scratch (FAST!)\n============================================================\nInitializing RotaryEmbedding with theta=10000.0 and ternary=True\n\n[RotaryEmbedding] Initialized with: dim=64, max_pos=2048, base=10000.0, ternary=True\n\nInitializing RotaryEmbedding with theta=10000.0 and ternary=True\n\n[RotaryEmbedding] Initialized with: dim=64, max_pos=2048, base=10000.0, ternary=True\n\nInitializing RotaryEmbedding with theta=10000.0 and ternary=True\n\n[RotaryEmbedding] Initialized with: dim=64, max_pos=2048, base=10000.0, ternary=True\n\nInitializing RotaryEmbedding with theta=10000.0 and ternary=True\n\n[RotaryEmbedding] Initialized with: dim=64, max_pos=2048, base=10000.0, ternary=True\n\nInitializing RotaryEmbedding with theta=10000.0 and ternary=True\n\n[RotaryEmbedding] Initialized with: dim=64, max_pos=2048, base=10000.0, ternary=True\n\nInitializing RotaryEmbedding with theta=10000.0 and ternary=True\n\n[RotaryEmbedding] Initialized with: dim=64, max_pos=2048, base=10000.0, ternary=True\n\nInitializing RotaryEmbedding with theta=10000.0 and ternary=True\n\n[RotaryEmbedding] Initialized with: dim=64, max_pos=2048, base=10000.0, ternary=True\n\nInitializing RotaryEmbedding with theta=10000.0 and ternary=True\n\n[RotaryEmbedding] Initialized with: dim=64, max_pos=2048, base=10000.0, ternary=True\n\nInitializing RotaryEmbedding with theta=10000.0 and ternary=True\n\n[RotaryEmbedding] Initialized with: dim=64, max_pos=2048, base=10000.0, ternary=True\n\nInitializing RotaryEmbedding with theta=10000.0 and ternary=True\n\n[RotaryEmbedding] Initialized with: dim=64, max_pos=2048, base=10000.0, ternary=True\n\nInitializing RotaryEmbedding with theta=10000.0 and ternary=True\n\n[RotaryEmbedding] Initialized with: dim=64, max_pos=2048, base=10000.0, ternary=True\n\nInitializing RotaryEmbedding with theta=10000.0 and ternary=True\n\n[RotaryEmbedding] Initialized with: dim=64, max_pos=2048, base=10000.0, ternary=True\n\nInitializing RotaryEmbedding with theta=10000.0 and ternary=True\n\n[RotaryEmbedding] Initialized with: dim=64, max_pos=2048, base=10000.0, ternary=True\n\nInitializing RotaryEmbedding with theta=10000.0 and ternary=True\n\n[RotaryEmbedding] Initialized with: dim=64, max_pos=2048, base=10000.0, ternary=True\n\nInitializing RotaryEmbedding with theta=10000.0 and ternary=True\n\n[RotaryEmbedding] Initialized with: dim=64, max_pos=2048, base=10000.0, ternary=True\n\nInitializing RotaryEmbedding with theta=10000.0 and ternary=True\n\n[RotaryEmbedding] Initialized with: dim=64, max_pos=2048, base=10000.0, ternary=True\n\nInitializing RotaryEmbedding with theta=10000.0 and ternary=True\n\n[RotaryEmbedding] Initialized with: dim=64, max_pos=2048, base=10000.0, ternary=True\n\nInitializing RotaryEmbedding with theta=10000.0 and ternary=True\n\n[RotaryEmbedding] Initialized with: dim=64, max_pos=2048, base=10000.0, ternary=True\n\n✅ Phase 1 weights loaded for fine-tuning\nMeshDiT(\n  (x_embedder): Linear(in_features=64, out_features=384, bias=True)\n  (t_embedder): TimestepEmbedder(\n    (mlp): Sequential(\n      (0): Linear(in_features=256, out_features=384, bias=True)\n      (1): SiLU()\n      (2): Linear(in_features=384, out_features=384, bias=True)\n    )\n  )\n  (y_embedder): TextEmbedder(\n    (embedding): Embedding(32001, 384)\n    (mlp): FullPrecisionMLP(\n      (gate_proj): Linear(in_features=384, out_features=2048, bias=False)\n      (down_proj): Linear(in_features=1024, out_features=384, bias=False)\n      (act_fn): SiLU()\n    )\n  )\n  (image_embedder): ImageLatentEmbedder(\n    (mlp): Sequential(\n      (0): Linear(in_features=16384, out_features=384, bias=True)\n      (1): SiLU()\n      (2): Linear(in_features=384, out_features=384, bias=True)\n    )\n  )\n  (dual_stream_blocks): ModuleList(\n    (0-5): 6 x DualStreamBlock(\n      (norm1): LayerNorm(384, elementwise_affine=False, eps=1e-06)\n      (norm2): LayerNorm(384, elementwise_affine=False, eps=1e-06)\n      (norm3): LayerNorm(384, elementwise_affine=False, eps=1e-06)\n      (norm4): LayerNorm(384, elementwise_affine=False, eps=1e-06)\n      (attn_x): HGRNBitAttention(\n        (i_proj): Linear(in_features=384, out_features=384, bias=False)\n        (f_proj): Linear(in_features=384, out_features=384, bias=False)\n        (g_proj): Linear(in_features=384, out_features=384, bias=False)\n        (g_norm): FusedRMSNormSwishGate()\n        (o_proj): Linear(in_features=384, out_features=384, bias=False)\n        (rotary_emb): RotaryEmbedding()\n      )\n      (attn_c): HGRNBitAttention(\n        (i_proj): Linear(in_features=384, out_features=384, bias=False)\n        (f_proj): Linear(in_features=384, out_features=384, bias=False)\n        (g_proj): Linear(in_features=384, out_features=384, bias=False)\n        (g_norm): FusedRMSNormSwishGate()\n        (o_proj): Linear(in_features=384, out_features=384, bias=False)\n        (rotary_emb): RotaryEmbedding()\n      )\n      (dual_cross_attn): DualCrossAttention(\n        (to_q_x): Linear(in_features=384, out_features=384, bias=False)\n        (to_kv_c): Linear(in_features=384, out_features=768, bias=False)\n        (lora_A_x): Linear(in_features=384, out_features=16, bias=False)\n        (lora_B_x): Linear(in_features=16, out_features=384, bias=False)\n        (proj_x): Linear(in_features=384, out_features=384, bias=False)\n        (to_q_c): Linear(in_features=384, out_features=384, bias=False)\n        (to_kv_x): Linear(in_features=384, out_features=768, bias=False)\n        (lora_A_c): Linear(in_features=384, out_features=16, bias=False)\n        (lora_B_c): Linear(in_features=16, out_features=384, bias=False)\n        (proj_c): Linear(in_features=384, out_features=384, bias=False)\n      )\n      (mlp): HGRNBitMLP(\n        (gate_proj): Linear(in_features=384, out_features=2048, bias=False)\n        (down_proj): Linear(in_features=1024, out_features=384, bias=False)\n        (act_fn): SiLU()\n      )\n      (mlp_c): HGRNBitMLP(\n        (gate_proj): Linear(in_features=384, out_features=2048, bias=False)\n        (down_proj): Linear(in_features=1024, out_features=384, bias=False)\n        (act_fn): SiLU()\n      )\n      (adaLN_modulation): FullPrecisionAdaLNConditioning(\n        (input_proj): Linear(in_features=384, out_features=384, bias=False)\n        (mlp): FullPrecisionMLP(\n          (gate_proj): Linear(in_features=384, out_features=2048, bias=False)\n          (down_proj): Linear(in_features=1024, out_features=384, bias=False)\n          (act_fn): SiLU()\n        )\n        (output_proj): Linear(in_features=384, out_features=4608, bias=True)\n        (norm): RMSNorm(4608, eps=1e-06)\n        (out_proj): Linear(in_features=4608, out_features=4608, bias=True)\n      )\n    )\n  )\n  (single_stream_blocks): ModuleList(\n    (0-5): 6 x SingleStreamBlock(\n      (norm1): LayerNorm(384, elementwise_affine=False, eps=1e-06)\n      (norm2): LayerNorm(384, elementwise_affine=False, eps=1e-06)\n      (attn): HGRNBitAttention(\n        (i_proj): Linear(in_features=384, out_features=384, bias=False)\n        (f_proj): Linear(in_features=384, out_features=384, bias=False)\n        (g_proj): Linear(in_features=384, out_features=384, bias=False)\n        (g_norm): FusedRMSNormSwishGate()\n        (o_proj): Linear(in_features=384, out_features=384, bias=False)\n        (rotary_emb): RotaryEmbedding()\n      )\n      (mlp): HGRNBitMLP(\n        (gate_proj): Linear(in_features=384, out_features=2048, bias=False)\n        (down_proj): Linear(in_features=1024, out_features=384, bias=False)\n        (act_fn): SiLU()\n      )\n      (adaLN_modulation): FullPrecisionAdaLNConditioning(\n        (input_proj): Linear(in_features=384, out_features=384, bias=False)\n        (mlp): FullPrecisionMLP(\n          (gate_proj): Linear(in_features=384, out_features=2048, bias=False)\n          (down_proj): Linear(in_features=1024, out_features=384, bias=False)\n          (act_fn): SiLU()\n        )\n        (output_proj): Linear(in_features=384, out_features=2304, bias=True)\n        (norm): RMSNorm(2304, eps=1e-06)\n        (out_proj): Linear(in_features=2304, out_features=2304, bias=True)\n      )\n    )\n  )\n  (final_layer): FinalLayer(\n    (norm_final): LayerNorm(384, elementwise_affine=False, eps=1e-06)\n    (linear): Linear(in_features=384, out_features=64, bias=True)\n    (adaLN_modulation): FullPrecisionAdaLNConditioning(\n      (input_proj): Linear(in_features=384, out_features=384, bias=False)\n      (mlp): FullPrecisionMLP(\n        (gate_proj): Linear(in_features=384, out_features=2048, bias=False)\n        (down_proj): Linear(in_features=1024, out_features=384, bias=False)\n        (act_fn): SiLU()\n      )\n      (output_proj): Linear(in_features=384, out_features=768, bias=True)\n      (norm): RMSNorm(768, eps=1e-06)\n      (out_proj): Linear(in_features=768, out_features=768, bias=True)\n    )\n  )\n)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.21.0"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20251129_175928-9qhos32h</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/sakibahmed2018go/mesh-dit-3d-generation/runs/9qhos32h' target=\"_blank\">MeshDiT-S-Phase1-Final</a></strong> to <a href='https://wandb.ai/sakibahmed2018go/mesh-dit-3d-generation' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/sakibahmed2018go/mesh-dit-3d-generation' target=\"_blank\">https://wandb.ai/sakibahmed2018go/mesh-dit-3d-generation</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/sakibahmed2018go/mesh-dit-3d-generation/runs/9qhos32h' target=\"_blank\">https://wandb.ai/sakibahmed2018go/mesh-dit-3d-generation/runs/9qhos32h</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2796' max='4107' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2796/4107 3:56:20 < 1:50:53, 0.20 it/s, Epoch 2.04/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>0.154400</td>\n      <td>0.154419</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.157600</td>\n      <td>0.151122</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.151600</td>\n      <td>0.148328</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.146900</td>\n      <td>0.146269</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>0.146300</td>\n      <td>0.146333</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"import os\nimport json\nimport shlex\nimport subprocess\nfrom datetime import datetime\n\nKAGGLE_USERNAME = \"sakibahmed2022\"\nKAGGLE_KEY = \"d0ec7fe8091a3906c6995568c11e9e58\" \nos.environ['KAGGLE_USERNAME'] = KAGGLE_USERNAME\nos.environ['KAGGLE_KEY'] = KAGGLE_KEY\n\nDESIRED_SLUG    = f\"{KAGGLE_USERNAME}/meshdit-trained-model\"\nLOCAL_DIR       = \"./mesh_dit_final\"\nTITLE           = \"MeshDiT-S-Model-0.08x\"\nVERSION_MSG     = f\"Upload final model – {datetime.utcnow().isoformat()} UTC\"\nPUBLIC          = False        \n\ndef write_metadata(folder: str, dataset_id: str):\n    meta = {\n        \"title\": TITLE,\n        \"id\": dataset_id,\n        \"licenses\": [{\"name\": \"CC-BY-4.0\"}],\n        \"description\": f\"{TITLE} – uploaded {datetime.utcnow().isoformat()} UTC\"\n    }\n    path = os.path.join(folder, \"dataset-metadata.json\")\n    with open(path, \"w\", encoding=\"utf-8\") as f:\n        json.dump(meta, f, indent=2)\n    print(f\"[meta] {path}\")\n\n\ndef try_api(folder: str, dataset_id: str, msg: str):\n    from kaggle.api.kaggle_api_extended import KaggleApi\n    api = KaggleApi()\n    api.authenticate()\n\n    try:\n        api.dataset_create_version(\n            folder=folder,\n            version_notes=msg,\n            convert_to_csv=False,\n            delete_old_versions=False,\n        )\n        print(\"[api] version created\")\n        return True, dataset_id\n    except Exception as e:\n        if \"403\" in str(e):\n            print(\"[api] 403 after file upload → will use CLI\")\n            return False, dataset_id\n        if \"does not exist\" in str(e).lower() or \"404\" in str(e):\n            print(\"[api] dataset missing → creating new\")\n            api.dataset_create_new(folder=folder, public=PUBLIC, convert_to_csv=False)\n            print(\"[api] new dataset created\")\n            return True, dataset_id\n        print(f\"[api] unexpected error: {e}\")\n        return False, None\n\n\ndef try_cli(folder: str, dataset_id: str, msg: str):\n    # 1. version\n    cmd = f\"kaggle datasets version -p {shlex.quote(folder)} -m {shlex.quote(msg)}\"\n    print(f\"[cli] version: {cmd}\")\n    p = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n    if p.returncode == 0:\n        print(\"[cli] version OK\")\n        return True, dataset_id\n\n    cmd = f\"kaggle datasets create -p {shlex.quote(folder)}\"\n    print(f\"[cli] create: {cmd}\")\n    p = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n    if p.returncode == 0:\n        print(\"[cli] create OK\")\n        return True, dataset_id\n\n    print(\"[cli] both commands failed\")\n    print(\"STDOUT:\", p.stdout)\n    print(\"STDERR:\", p.stderr)\n    return False, None\n\ndef upload():\n    if not os.path.isdir(LOCAL_DIR):\n        raise FileNotFoundError(LOCAL_DIR)\n\n    files = [f for f in os.listdir(LOCAL_DIR) if os.path.isfile(os.path.join(LOCAL_DIR, f))]\n    print(\"[main] files:\", files)\n\n    dataset_id = DESIRED_SLUG\n    ts = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n    alt = f\"{KAGGLE_USERNAME}/meshdit-trained-model-{ts}\"\n\n    try:\n        from kaggle.api.kaggle_api_extended import KaggleApi\n        KaggleApi().authenticate()\n        KaggleApi().dataset_list_files(dataset_id)   # raises if missing\n        print(f\"[slug] '{dataset_id}' exists → using timestamped\")\n        dataset_id = alt\n    except Exception:\n        pass   # keep desired slug\n\n    write_metadata(LOCAL_DIR, dataset_id)\n\n    # 1. API\n    print(\"[main] trying Python API …\")\n    ok, final_id = try_api(LOCAL_DIR, dataset_id, VERSION_MSG)\n    if ok:\n        url = f\"https://www.kaggle.com/datasets/{final_id}\"\n        print(f\"[SUCCESS] API → {url}\")\n        return url\n\n    # 2. CLI\n    print(\"[main] API failed → CLI fallback …\")\n    ok, final_id = try_cli(LOCAL_DIR, dataset_id, VERSION_MSG)\n    if ok:\n        url = f\"https://www.kaggle.com/datasets/{final_id}\"\n        print(f\"[SUCCESS] CLI → {url}\")\n        return url\n\n    raise RuntimeError(\"Both API and CLI failed\")\n\n# --------------------------------------------------------------\n# 7. RUN\n# --------------------------------------------------------------\nif __name__ == \"__main__\":\n    final_url = upload()\n    print(\"\\n=== FINAL URL ===\")\n    print(final_url)\n    print(\"\\nDownload with:\")\n    print(f\"kaggle datasets download -d {final_url.split('/')[-2]}/{final_url.split('/')[-1]}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}